{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code aims at converting tensor back to text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import collections\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "import string\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./saved_models/temp55/', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = [set(), set()]\n",
    "pred = [None, None]\n",
    "index_mapping = [[], []]\n",
    "for side, key in enumerate(['pos', 'neg']):\n",
    "    pred[side] = torch.load('./pred_result/result_gaku_{}_post_mask'.format(key))\n",
    "    index = [_.item() for _ in pred[side]['index']]\n",
    "    \n",
    "    with open('./mapping_clean_{}_single'.format(key)) as f:\n",
    "        for line in f:\n",
    "            index_mapping[side].append(line.strip())\n",
    "    \n",
    "    for i in range(1, len(index)):\n",
    "        for _ in range(index[i-1]+1, index[i]):\n",
    "            t = index_mapping[side][_]\n",
    "            check[side].add( int(t.split('_')[0]))\n",
    "\n",
    "check = (check[0] or check[1])\n",
    "len(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_type = [None, None]\n",
    "for side, key in enumerate(['pos', 'neg']):\n",
    "    pred_type[side] = torch.load('./pred_result/type_gaku_{}_post'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkandvote(sent, recovers, bios, dtypes):\n",
    "    mapping_bio = ['B', 'I', 'O']\n",
    "    mapping_type = ['P', 'C']\n",
    "    \n",
    "    \n",
    "    recover_sent, recover_bio = [], []\n",
    "    adu_type = []\n",
    "    count = 0\n",
    "    \n",
    "    last = torch.rand(2)\n",
    "    for index, (word, r, bio, dtype) in enumerate(zip(sent[1:], recovers[1:], bios[1:-1], dtypes[1:])):\n",
    "        if(r and word[:2]=='##'):\n",
    "            if(bio==0):\n",
    "                adu_type.append(\n",
    "                    mapping_type[last.argmax(-1).item()]\n",
    "                )\n",
    "                last = torch.zeros_like(dtype)\n",
    "                recover_bio[-1] = mapping_bio[bio]\n",
    "                count += 1\n",
    "                \n",
    "            recover_sent[-1] += word.split('#')[-1]\n",
    "        else:\n",
    "            if(bio==0):\n",
    "                adu_type.append(\n",
    "                    mapping_type[last.argmax(-1).item()]\n",
    "                )\n",
    "                last = torch.zeros_like(dtype)\n",
    "                count += 1\n",
    "\n",
    "            recover_sent.append(word)\n",
    "            recover_bio.append(mapping_bio[bio])\n",
    "\n",
    "        if(bio<=1):\n",
    "            last += dtype\n",
    "    else:\n",
    "        adu_type.append(\n",
    "            mapping_type[last.argmax(-1).item()]\n",
    "        )\n",
    "    assert count==(len(adu_type)-1)\n",
    "    return recover_sent, recover_bio, adu_type[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "recover = [\n",
    "    {'sent':[], 'bio':[], 'index':[],'type':[]},\n",
    "    {'sent':[], 'bio':[], 'index':[],'type':[]}\n",
    "]\n",
    "\n",
    "for side in range(2):\n",
    "    for index in range(len(pred[side]['id'])):\n",
    "        temp = checkandvote(\n",
    "                        tokenizer.convert_ids_to_tokens(pred[side]['id'][index].tolist()), \n",
    "                        pred[side]['recover'][index].tolist(), pred[side]['bio'][index],\n",
    "                        torch.tensor(pred_type[side]['type'][index]).softmax(-1)\n",
    "                       )\n",
    "        \n",
    "        recover[side]['sent'].append(' '.join(temp[0]))\n",
    "        recover[side]['bio'].append(temp[1])\n",
    "        recover[side]['type'].append(temp[2])\n",
    "        recover[side]['index'].append(pred[side]['index'][index].item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_recover = copy.deepcopy(recover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate sequence B\n",
    "for side in range(2):\n",
    "    for r_index in range(len(recover[side]['bio'])):\n",
    "        for index in range( len(recover[side]['bio'][r_index])-1, 0, -1):\n",
    "            if( recover[side]['bio'][r_index][index]=='B' and recover[side]['bio'][r_index][index-1]=='B' ):\n",
    "                recover[side]['bio'][r_index][index]='I'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removec last punctation\n",
    "checklist = ['and', 'or']\n",
    "for side in range(2):\n",
    "    for r_index in range(len(recover[side]['bio'])):\n",
    "        for index in range( len(recover[side]['bio'][r_index])-1):\n",
    "            if( recover[side]['bio'][r_index][index]=='I' and recover[side]['bio'][r_index][index+1]!='I'):\n",
    "                sent = recover[side]['sent'][r_index].split()\n",
    "                if((sent[index] in string.punctuation) or (sent[index] in checklist)):\n",
    "                    recover[side]['bio'][r_index][index]='O'\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find next token is that\n",
    "def checkandcollect(recover, key):\n",
    "    rm_stat = collections.defaultdict(int)\n",
    "\n",
    "    for side in range(2):\n",
    "        for r_index in range(len(recover[side]['bio'])):\n",
    "            buffer = []\n",
    "            sent = recover[side]['sent'][r_index].split()\n",
    "\n",
    "            for index in range( len(recover[side]['bio'][r_index])):\n",
    "                if(recover[side]['bio'][r_index][index]=='O'):\n",
    "                    if(len(buffer) and sent[index]==key):\n",
    "                        rm_stat[' '.join(buffer)] += 1\n",
    "                        buffer = []\n",
    "                else:\n",
    "                    buffer.append(sent[index])\n",
    "                    \n",
    "    return rm_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_sent = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_sent['that'] = checkandcollect(recover, 'that')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_sent['which'] = checkandcollect(recover, 'which')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_sent['whether'] = checkandcollect(recover, 'whether')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removebio(recover, rm_set, len_limit=4):\n",
    "    for side in range(2):\n",
    "        for index in range(len(recover[side]['sent'])):\n",
    "            # index = 561\n",
    "            try:\n",
    "                sent, bio = recover[side]['sent'][index].split(), recover[side]['bio'][index]\n",
    "                new_sent, new_bio, new_type = [], [], []\n",
    "\n",
    "                sent.append('')\n",
    "                bio.append('O')\n",
    "\n",
    "                buffer = []\n",
    "                type_index= 0\n",
    "\n",
    "                for windex in range(len(sent)):\n",
    "                    new_sent.append( sent[windex] )\n",
    "\n",
    "                    if( bio[windex]=='O'):\n",
    "                        if( len(buffer) ):\n",
    "                            check_sent = ' '.join(buffer)\n",
    "                            if((len(buffer) <= len_limit) and (check_sent in rm_set)):\n",
    "                                # convert these data to others\n",
    "                                new_type.pop()\n",
    "                                new_bio.extend( ['O']*(len(buffer)) )\n",
    "                            else:\n",
    "                                new_bio.append('B')\n",
    "                                new_bio.extend( ['I']*(len(buffer)-1) )\n",
    "                            buffer = []\n",
    "\n",
    "                        new_bio.append( bio[windex] )\n",
    "                    else:\n",
    "                        if( bio[windex] == 'B'):\n",
    "                            new_type.append( recover[side]['sent'][index][type_index] )\n",
    "                            type_index += 1\n",
    "                        buffer.append(sent[windex])        \n",
    "\n",
    "                sent.pop()\n",
    "                bio.pop()\n",
    "\n",
    "                recover[side]['sent'][index] = ' '.join(new_sent)\n",
    "                recover[side]['bio'][index] = new_bio\n",
    "                recover[side]['type'][index] = new_type\n",
    "            except:\n",
    "                print(side, index)\n",
    "                None+1\n",
    "            #break\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 116420\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-fcdb5d52602f>\u001b[0m in \u001b[0;36mremovebio\u001b[0;34m(recover, rm_set, len_limit)\u001b[0m\n\u001b[1;32m     22\u001b[0m                                 \u001b[0;31m# convert these data to others\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                 \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                                 \u001b[0mnew_bio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-14c43640990f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrecover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_recover\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mremovebio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecover\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrm_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'that'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-fcdb5d52602f>\u001b[0m in \u001b[0;36mremovebio\u001b[0;34m(recover, rm_set, len_limit)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mNone\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;31m#break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "recover = copy.deepcopy(temp_recover)\n",
    "removebio(recover, rm_sent['that'], len_limit=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "it I\n",
      "was I\n",
      "not I\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "side, index = 1, 116420\n",
    "\n",
    "print(recover[side]['type'][index])\n",
    "for word, bio in zip(recover[side]['sent'][index].split(),recover[side]['bio'][index]):\n",
    "    print(word, bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'it',\n",
       " 'was',\n",
       " 'not',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'greed',\n",
       " 'and',\n",
       " 'other',\n",
       " 'traits',\n",
       " 'associated',\n",
       " 'with',\n",
       " '\"',\n",
       " 'human',\n",
       " 'nature',\n",
       " '\"',\n",
       " 'are',\n",
       " 'learned',\n",
       " 'values',\n",
       " ',',\n",
       " 'not',\n",
       " 'evolutionary',\n",
       " 'ones',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(pred[side]['id'][index].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 2, 2]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[side]['bio'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'is', 'that', 'tasteless']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 867\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'is', 'that', 'tasteless']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 1031\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'is', 'that', 'tasteless']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 1328\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'is', 'that', 'tasteless']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 1623\n",
      "\n",
      "2\n",
      "['the', 'problem', 'is', 'that', ',', 'even', 'though', 'you']\n",
      "['B', 'I', 'I', 'O', 'O', 'O', 'O', 'B']\n",
      "0 6462\n",
      "\n",
      "5\n",
      "['the', 'problem', 'is', 'that', 'the', 'media', 'take', 'things']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 7062\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'we', 'live', 'in', 'the']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 8566\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'angry', 'people', 'can', 'be']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 8884\n",
      "\n",
      "2\n",
      "['the', 'problem', 'is', 'that', 'nobody', ',', 'rich', 'enough']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 21972\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', ',', 'when', 'black', 'people', 'want']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 38138\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'many', 'obese', 'and', 'overweight']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 46554\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'many', 'obese', 'and', 'overweight']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 46989\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'while', 'there', 'is', 'a']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 60179\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'these', 'religions', 'often', 'explicitly']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 61014\n",
      "\n",
      "3\n",
      "['the', 'problem', 'is', 'that', 'i', \"'\", 'm', 'having']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 66239\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 88230\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 88796\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 89270\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 89441\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 89678\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 89958\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 90349\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 90810\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'no', 'one', 'can', 'truly']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 93168\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'its', 'overuse', 'has', 'become']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 109097\n",
      "\n",
      "1\n",
      "['the', 'problem', 'is', 'that', 'if', 'adherents', 'do', \"'\"]\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 118432\n",
      "\n",
      "1\n",
      "['the', 'problem', 'is', 'that', 'if', 'adherents', 'do', \"'\"]\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 118790\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', ',', 'unless', 'you', 'have']\n",
      "['B', 'I', 'I', 'O', 'O', 'B', 'I', 'I']\n",
      "0 121580\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', ',', 'as', 'a', 'general']\n",
      "['B', 'I', 'I', 'O', 'O', 'B', 'I', 'I']\n",
      "0 124316\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'without', 'predators', 'keeping', 'prey']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 143291\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'you', 'are', ',', 'for']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 146453\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'i', 'look', 'at', '\"']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 154222\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'in', 'the', 'us', 'we']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 165989\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', ',', 'as', 'the', 'nasa', 'graph']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 171113\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'people', 'tend', 'to', 'over']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 177587\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'you', 'can', \"'\", 't']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 190163\n",
      "\n",
      "4\n",
      "['the', 'problem', 'is', 'that', 'a', 'love', 'story', 'does']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 193093\n",
      "\n",
      "1\n",
      "['the', 'problem', 'is', 'that', 'ludicrous', 'views', 'are', 'not']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 203474\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'our', 'stomachs', 'get', 'full']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 221062\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'population', 'growth', 'and', 'stability']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 221448\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'idea', 'of', '\"']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 235947\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', ',', 'people', 'who', 'eat', 'meat']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 236192\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'most', 'countries', 'do', \"'\"]\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 239167\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'introduces', 'instability', 'into']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 250666\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'introduces', 'instability', 'into']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 251042\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'religious', 'arguments', 'of']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 251385\n",
      "\n",
      "2\n",
      "['the', 'problem', 'is', 'that', 'the', 'cost', 'of', 'children']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 251580\n",
      "\n",
      "3\n",
      "['the', 'problem', 'is', 'that', 'the', 'cost', 'of', 'children']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 251620\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'this', 'proposal', 'wont', 'achieve']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 261980\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', ',', 'fair', 'or', 'not']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 267475\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'you', 'just', 'kinda', 'brush']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 279878\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'other', 'majors', 'have', 'turned']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 282663\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'most', 'overqualified', 'people', 'apply']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 284377\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'training', 'costs', 'mean', 'the']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 284383\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'you', 'made', 'a', 'blanket']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 294833\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'sexual', 'crimes', 'have', 'a']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 307182\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'rape', 'by', 'strangers', 'if']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "0 315197\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'is', 'that', 'tasteless']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 537\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'is', 'that', 'tasteless']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 960\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'is', 'that', 'tasteless']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 1159\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'is', 'that', 'tasteless']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 1450\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'is', 'that', 'tasteless']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 1915\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', ',', 'no', 'matter', 'what']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 6937\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', '\"', 'shaming', '\"', 'as']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 12937\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'they', 'are', 'compelling', 'you']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 18557\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'political', 'parties', 'are', 'inevitable']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 35765\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'political', 'parties', 'are', 'inevitable']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 36163\n",
      "\n",
      "1\n",
      "['the', 'problem', 'is', 'that', 'in', 'lot', 'of', 'the']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 43267\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'american', 'psychology', 'association']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 50588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'a', 'man', 'can', 'be']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 57126\n",
      "\n",
      "2\n",
      "['the', 'problem', 'is', 'that', 'your', 'way', 'of', 'viewing']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 61099\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'there', 'is', 'only', 'one']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 67787\n",
      "\n",
      "3\n",
      "['the', 'problem', 'is', 'that', 'i', \"'\", 'm', 'having']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 71023\n",
      "\n",
      "3\n",
      "['the', 'problem', 'is', 'that', 'i', \"'\", 'm', 'having']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 71251\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'information', 'exists', ',', 'just']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 72532\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'if', 'you', 'support', 'paid']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 73388\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'there', 'are', 'many', 'different']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 78389\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 93955\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 94517\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 94955\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 95138\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 95529\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 95893\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 96338\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'government', 'which', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 96754\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', \"'\", 's', 'probably']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 105421\n",
      "\n",
      "1\n",
      "['the', 'problem', 'is', 'that', 'none', 'of', 'these', 'issues']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 122578\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'there', 'are', 'no', 'art']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 125592\n",
      "\n",
      "1\n",
      "['the', 'problem', 'is', 'that', 'if', 'adherents', 'do', \"'\"]\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 126439\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'settling', 'those', 'areas', 'of']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 126513\n",
      "\n",
      "1\n",
      "['the', 'problem', 'is', 'that', 'if', 'adherents', 'do', \"'\"]\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 126890\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', ',', 'unless', 'you', 'have']\n",
      "['B', 'I', 'I', 'O', 'O', 'B', 'I', 'I']\n",
      "1 129869\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'our', 'debt', 'is', 'so']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 137116\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'there', 'are', 'people', 'who']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 141601\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'option', 'a', 'leads', 'to']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 150084\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'without', 'predators', 'keeping', 'prey']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 152897\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'same', 'can', 'be']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 164479\n",
      "\n",
      "1\n",
      "['the', 'problem', 'is', 'that', 'after', 'i', 'read', 'the']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 202530\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'people', 'are', 'relatively', 'good']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 204676\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'superman', 'predates', '\"', 'the']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 220281\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'the', 'amount', 'of', 'co2']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 222692\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'capitcalism', 'is', 'the', 'best']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 227125\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'our', 'stomachs', 'get', 'full']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 236691\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'what', 'constitutes', 'proof', 'is']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 241752\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'your', 'reasoning', 'is', 'true']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 246105\n",
      "\n",
      "3\n",
      "['the', 'problem', 'is', 'that', 'humans', 'are', \"'\", 't']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 253976\n",
      "\n",
      "1\n",
      "['the', 'problem', 'is', 'that', 'representative', 'democracy', 'created', 'the']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 257065\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'you', 'act', 'like', 'there']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 261835\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'nowadays', ',', 'population', 'growth']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 266344\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'introduces', 'instability', 'into']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 269289\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'introduces', 'instability', 'into']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 269668\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'while', 'taxation', 'is', 'all']\n",
      "['B', 'I', 'I', 'O', 'O', 'B', 'I', 'I']\n",
      "1 269800\n",
      "\n",
      "2\n",
      "['the', 'problem', 'is', 'that', 'in', 'the', 'town', 'i']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 273647\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'there', 'is', 'nothing', 'conservative']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 277684\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'people', 'who', 'wanted', 'to']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 278135\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'people', 'are', 'learning', 'just']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 278801\n",
      "\n",
      "1\n",
      "['the', 'problem', 'is', 'that', 'these', 'promises', 'are', 'made']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 282061\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'a', 'lot', 'of', 'people']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 291735\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'critics', 'are', 'approaching', 'movies']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 294710\n",
      "\n",
      "12\n",
      "['the', 'problem', 'is', '\"', 'how', 'do', 'we', 'implement']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 297370\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'we', 'do', \"'\", 't']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 298509\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'other', 'majors', 'have', 'turned']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 304187\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'most', 'overqualified', 'people', 'apply']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 306029\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'training', 'costs', 'mean', 'the']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 306062\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'it', 'is', 'not', 'the']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 312342\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'we', 'are', 'affected', 'by']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 317932\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'you', 'made', 'a', 'blanket']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 318013\n",
      "\n",
      "2\n",
      "['the', 'problem', 'is', 'that', 'the', 'types', 'of', 'animals']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 320276\n",
      "\n",
      "9\n",
      "['the', 'problem', 'is', 'that', 'i', 'dont', 'know', 'how']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 323231\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'most', 'people', 'who', 'are']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 328938\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'sexual', 'crimes', 'have', 'a']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 332350\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'property', 'rights', 'does', 'not']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 333826\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'those', 'without', 'any', 'working']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 335711\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'those', 'things', 'can', 'cost']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 336617\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'those', 'things', 'can', 'cost']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 336674\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'those', 'things', 'can', 'cost']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 336733\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'rape', 'by', 'strangers', 'if']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 341610\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'rape', 'by', 'strangers', 'if']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 341658\n",
      "\n",
      "0\n",
      "['the', 'problem', 'is', 'that', 'rape', 'by', 'strangers', 'if']\n",
      "['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I']\n",
      "1 341690\n"
     ]
    }
   ],
   "source": [
    "def checkandprint(check, arr, post, bios):\n",
    "    arr = ' '.join(arr)\n",
    "    key = \"the problem is\"\n",
    "    l = len(key.split())\n",
    "    if(arr == key):\n",
    "        for index in range(len(post)-l):\n",
    "            if(' '.join(post[index:index+l]) == key and bios[index]=='B' and bios[index+l]=='O'):\n",
    "                print()\n",
    "                print(index)\n",
    "                print(post[index:index+8])\n",
    "                print(bios[index:index+8])\n",
    "                \n",
    "                check[''.join(bios[index:index+4])] += 1\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "check = collections.defaultdict(int)\n",
    "length = 8\n",
    "for side in range(2):\n",
    "    for index in range(len(recover[side]['sent'])):\n",
    "        buffer = []\n",
    "        for word, bio in zip(recover[side]['sent'][index].split(),recover[side]['bio'][index]):\n",
    "            if(bio == 'B'):\n",
    "                if(len(buffer)):\n",
    "                    if(checkandprint(check, buffer[:length], recover[side]['sent'][index].split(),recover[side]['bio'][index])):\n",
    "                        print(side, index)\n",
    "                    \n",
    "            if(bio == 'B' or bio == 'I'):\n",
    "                buffer.append(word)\n",
    "        else:\n",
    "            if(bio == 'B'):\n",
    "                if(len(buffer)):\n",
    "                    if(checkandprint(check, buffer[:length], recover[side]['sent'][index].split(),recover[side]['bio'][index])):\n",
    "                        print(side, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 8\n",
    "stat = []\n",
    "for _ in range(2):\n",
    "    stat.append([collections.defaultdict(int) for _ in range(length)])\n",
    "    \n",
    "def adddict(dataset, words, length, front=True):\n",
    "    l = min(len(buffer), length)\n",
    "    if(front):\n",
    "        dataset[l-1][' '.join(buffer[:l])] += 1\n",
    "    else:\n",
    "        dataset[l-1][' '.join(buffer[-l:])] += 1\n",
    "    \n",
    "for side in range(2):\n",
    "    for index in range(len(recover[side]['sent'])):\n",
    "        buffer = []\n",
    "        for word, bio in zip(recover[side]['sent'][index].split(),recover[side]['bio'][index]):\n",
    "            if(bio == 'B'):\n",
    "                if(len(buffer)):\n",
    "                    adddict(stat[0], buffer, length, True)\n",
    "                    adddict(stat[1], buffer, length, False)\n",
    "                                        \n",
    "            if(bio == 'B' or bio == 'I'):\n",
    "                buffer.append(word)\n",
    "        else:\n",
    "            if(bio == 'B'):\n",
    "                if(len(buffer)):\n",
    "                    adddict(stat[0], buffer, length, True)\n",
    "                    adddict(stat[1], buffer, length, False)\n",
    "                    \n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(length):\n",
    "        stat[i][j] = sorted([(key, value) for key, value in stat[i][j].items()], key = lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the problem 133\n",
      "one problem 5\n",
      "my problem 3\n",
      "problem with 1\n",
      "a problem 1\n",
      "problem is 1\n",
      "no problem 1\n",
      "the problem is 140\n",
      "my problem is 9\n",
      "one problem is 5\n",
      "this is problematic 3\n",
      "the problem with 2\n",
      "the problem , 2\n",
      "a major problem 1\n",
      "the big problem 1\n",
      "a problem is 1\n",
      "a crucial problem 1\n",
      "these are problems 1\n",
      "big problem here 1\n",
      "the main problem 1\n",
      "there are problems 1\n",
      "it becomes problematic 1\n",
      "your problem is 1\n",
      "not a problem 1\n",
      "this is the problem 26\n",
      "this is a problem 18\n",
      "the problem here is 8\n",
      "the problem with this 7\n",
      "there is a problem 6\n",
      "the main problem is 4\n",
      "the problem with that 4\n",
      "problems with mars 1 4\n",
      "this is my problem 4\n",
      "that is a problem 4\n",
      "my main problem is 2\n",
      "overpopulation is a problem 2\n",
      "the problem of evil 2\n",
      "obesity causes health problems 2\n",
      "this is your problem 2\n",
      "that is their problem 2\n",
      "here is my problem 2\n",
      "what problems with violence 2\n",
      "the problem is regional 2\n",
      "the problem was ricky 2\n",
      "there is your problem 2\n",
      "the only problem is 2\n",
      "inflation is a problem 2\n",
      "the problem is support 2\n",
      "it is a problem 2\n",
      "the problem is that 2\n",
      "the problems were addressed 2\n",
      "health problems have associated 1\n",
      "the real problem is 1\n",
      "it goes the problem 1\n",
      "the problem is risk 1\n",
      "the problem is not 1\n",
      "i have memory problems 1\n",
      "the problem is huge 1\n",
      "my problem with that 1\n",
      "trump represents the problem 1\n",
      "it is incredibly problematic 1\n",
      "the problem lies in 1\n",
      "speed is a problem 1\n",
      "my problem with it 1\n",
      "lots of problems there 1\n",
      "the handshake is problematic 1\n",
      "i see a problem 1\n",
      "i find it problematic 1\n",
      "the fundamental problem is 1\n",
      "the problem is deceit 1\n",
      "the other problem is 1\n",
      "the problem is culture 1\n",
      "i do the problem 1\n",
      "there is another problem 1\n",
      "your problem is illinois 1\n",
      "the bigger problem is 1\n",
      "those have some problems 1\n",
      "that is the problem 1\n",
      "adjustment would be problematic 1\n",
      "people are having problems 1\n",
      "the problem with clonability 1\n",
      "monopsony is the problem 1\n",
      "closed source software problem 1\n",
      "word problems are hard 1\n",
      "the problem comes down 1\n",
      "just an engineering problem 1\n",
      "they are trivial problems 1\n",
      "issues of consent are problematic 35\n",
      "the biggest problem is overpopulation 31\n",
      "my problem stems from the 16\n",
      "that ' s the problem 11\n",
      "my problem with this is 11\n",
      "what ' s the problem 8\n",
      "the problem with this is 7\n",
      "the problem with that is 5\n",
      "there ' s a problem 4\n",
      "the problem with this critique 4\n",
      "a problem with this is 4\n",
      "it ' s a problem 3\n",
      "addiction is a real problem 3\n",
      "the problem with this , 3\n",
      "it does prevent one problem 3\n",
      "i have a problem now 2\n",
      "the same problem arisesinfinitely worse 2\n",
      "as for the overall problem 2\n",
      "congressional meddling is a problem 2\n",
      "that ' s my problem 2\n",
      "counterfeiting would be a problem 2\n",
      "there ' s your problem 2\n",
      "this particular case is problematic 2\n",
      "this is not a problem 2\n",
      "current day america has problems 2\n",
      "the problem with reddit , 2\n",
      "this also creates a problem 2\n",
      "why is that my problem 2\n",
      "one of the problems is 2\n",
      "instead of solving all problems 2\n",
      "extreme first world problem here 2\n",
      "my main problem is , 2\n",
      "diversity and heterogeneity creates problems 2\n",
      "how problematic is it really 2\n",
      "this is a big problem 2\n",
      "there are other problems too 2\n",
      "my analogy fits the problem 2\n",
      "there are problems with trump 2\n",
      "energy is not the problem 2\n",
      "micromanaging markets is a problem 2\n",
      "your second it gets problematic 2\n",
      "the problem i have is 2\n",
      "that is a problem too 2\n",
      "the problem is in causation 2\n",
      "there is a problem with 1\n",
      "containment is the other problem 1\n",
      "my problem with your view 1\n",
      "this is a mental problem 1\n",
      "that is also problematic to 1\n",
      "your argument is fundamentally problematic 1\n",
      "this is your main problem 1\n",
      "the only two problems with 1\n",
      "your alien analogy is problematic 1\n",
      "we see the problem is 1\n",
      "the capitalism had its problems 1\n",
      "this would be a problem 1\n",
      "it has problems beyond that 1\n",
      "the problem i see is 1\n",
      "why is this a problem 1\n",
      "brigading is indeed a problem 1\n",
      "these are not actual problems 1\n",
      "the problem with legalising prostitution 1\n",
      "all problems are not equal 1\n",
      "we have a problem here 1\n",
      "there are problems within society 1\n",
      "this wraps the problem nicely 1\n",
      "this is exactly the problem 1\n",
      "this will be a problem 1\n",
      "the problem with prejudices is 1\n",
      "the investors problem is huge 1\n",
      "this is a real problem 1\n",
      "the second problem is simply 1\n",
      "it may create more problems 1\n",
      "we should solve the problem 1\n",
      "there are problems with trafficking 1\n",
      "this problem has remained static 1\n",
      "the problem is not islam 1\n",
      "money was not the problem 1\n",
      "removing votes is also problematic 1\n",
      "the problem with your idea 1\n",
      "the problem with tsu is 1\n",
      "is not necessarily a problem 1\n",
      "the problem with your argument 1\n",
      "the beard was a problem 1\n",
      "it ' s our problem 1\n",
      "there is the problem with 1\n",
      "the other problem with this 1\n",
      "in terms of prioritizing problems 1\n",
      "part of the problem is 1\n",
      "this is certainly a problem 1\n",
      "this is the problem , 1\n",
      "religious beliefs are a problem 1\n",
      "this is the essential problem 1\n",
      "i still have these problems 1\n",
      "the problem with conspiracy theories 1\n",
      "europe has a migrant problem 1\n",
      "we fell my final problem 1\n",
      "why is that a problem 1\n",
      "part of my problem is 1\n",
      "this is the whole problem 1\n",
      "the problem lies in interpretation 1\n",
      "this is a temporary problem 1\n",
      "both can be interpreted problematically 1\n",
      "the only big problem is 1\n",
      "there ' s the problem 1\n",
      "resource scarcity is another problem 1\n",
      "do you see the problem 1\n",
      "superman is not the problem 1\n",
      "cgi has a believability problem 1\n",
      "how are these problems solved 1\n",
      "it also creates a problem 1\n",
      "absolute meritocracy is a problem 1\n",
      "psychology can have similar problems 1\n",
      "the problem here is clearly 1\n",
      "this is a bit problematic 1\n"
     ]
    }
   ],
   "source": [
    "for _ in stat[0][:5]:\n",
    "    for key, val in _:\n",
    "        if('problem' in key):\n",
    "            print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"i '\", 1456),\n",
       " ('this is', 1065),\n",
       " (\"it '\", 643),\n",
       " ('i do', 566),\n",
       " (\"let '\", 440),\n",
       " ('it is', 391),\n",
       " ('i am', 330),\n",
       " ('i would', 316),\n",
       " ('if you', 281),\n",
       " (\"that '\", 279),\n",
       " ('you are', 248),\n",
       " (\"you '\", 247),\n",
       " ('i can', 228),\n",
       " ('i was', 227),\n",
       " ('there are', 207),\n",
       " ('it seems', 207),\n",
       " ('i have', 206),\n",
       " ('i agree', 201),\n",
       " ('there is', 155),\n",
       " ('i still', 141),\n",
       " ('the problem', 133),\n",
       " (\"there '\", 129),\n",
       " ('in the', 128),\n",
       " ('you have', 117),\n",
       " ('the point', 112),\n",
       " ('i did', 108),\n",
       " ('i disagree', 107),\n",
       " ('i feel', 104),\n",
       " ('it does', 100),\n",
       " ('do you', 98),\n",
       " ('i should', 97),\n",
       " ('i really', 95),\n",
       " ('if i', 94),\n",
       " ('you can', 86),\n",
       " ('the fact', 85),\n",
       " ('why not', 82),\n",
       " ('i could', 75),\n",
       " ('can you', 74),\n",
       " ('are you', 71),\n",
       " ('some people', 70),\n",
       " ('it was', 66),\n",
       " ('it would', 66),\n",
       " ('my point', 64),\n",
       " ('i will', 63),\n",
       " ('you make', 62),\n",
       " ('these are', 61),\n",
       " ('the issue', 60),\n",
       " ('my view', 58),\n",
       " ('that is', 56),\n",
       " ('the most', 54),\n",
       " ('this one', 52),\n",
       " ('i actually', 51),\n",
       " ('i wont', 50),\n",
       " ('i had', 50),\n",
       " ('the only', 50),\n",
       " (\"here '\", 49),\n",
       " ('as a', 49),\n",
       " ('you could', 48),\n",
       " ('right now', 48),\n",
       " ('we can', 48),\n",
       " ('i never', 47),\n",
       " ('my argument', 47),\n",
       " ('your argument', 45),\n",
       " ('you do', 45),\n",
       " ('you might', 44),\n",
       " (\"do '\", 44),\n",
       " ('what i', 42),\n",
       " ('it sounds', 42),\n",
       " ('i personally', 42),\n",
       " ('i see', 41),\n",
       " ('at the', 41),\n",
       " ('let me', 39),\n",
       " ('you did', 38),\n",
       " ('this was', 38),\n",
       " ('i may', 38),\n",
       " ('at this', 38),\n",
       " ('in this', 38),\n",
       " ('could you', 37),\n",
       " ('in my', 36),\n",
       " ('they do', 35),\n",
       " ('look at', 35),\n",
       " ('the reason', 35),\n",
       " ('they are', 35),\n",
       " ('that was', 34),\n",
       " ('when i', 34),\n",
       " ('the argument', 32),\n",
       " ('the idea', 32),\n",
       " ('google does', 32),\n",
       " ('would you', 30),\n",
       " ('it may', 30),\n",
       " ('what is', 30),\n",
       " ('by and', 30),\n",
       " ('if this', 30),\n",
       " ('it has', 29),\n",
       " ('if we', 29),\n",
       " ('i completely', 29),\n",
       " ('most of', 29),\n",
       " ('think about', 29),\n",
       " ('it also', 28),\n",
       " ('you may', 28),\n",
       " ('the answer', 28),\n",
       " ('you say', 28),\n",
       " ('i also', 28),\n",
       " ('this may', 28),\n",
       " ('here is', 28),\n",
       " ('if my', 28),\n",
       " ('facebook and', 28),\n",
       " ('for us', 28),\n",
       " ('many people', 27),\n",
       " ('there was', 27),\n",
       " ('if a', 27),\n",
       " ('this means', 27),\n",
       " ('no one', 27),\n",
       " ('i just', 27),\n",
       " ('the solution', 27),\n",
       " ('most people', 26),\n",
       " ('i understand', 26),\n",
       " ('i saw', 26),\n",
       " ('what about', 25),\n",
       " ('the second', 24),\n",
       " ('the main', 24),\n",
       " ('for those', 24),\n",
       " ('now in', 24),\n",
       " ('your point', 23),\n",
       " ('i already', 23),\n",
       " ('those are', 23),\n",
       " ('i like', 23),\n",
       " ('if your', 23),\n",
       " ('i definitely', 23),\n",
       " ('women have', 23),\n",
       " ('the difference', 22),\n",
       " (\"what '\", 22),\n",
       " ('you should', 22),\n",
       " ('you also', 22),\n",
       " ('it looks', 22),\n",
       " ('i might', 21),\n",
       " ('it could', 21),\n",
       " ('a lot', 21),\n",
       " ('i fully', 21),\n",
       " ('in your', 20),\n",
       " ('if the', 20),\n",
       " ('after doing', 20),\n",
       " ('i came', 19),\n",
       " ('your view', 19),\n",
       " ('we are', 19),\n",
       " ('if that', 19),\n",
       " ('copyright protection', 18),\n",
       " ('one of', 18),\n",
       " ('i read', 18),\n",
       " ('i now', 18),\n",
       " ('there have', 18),\n",
       " ('this has', 18),\n",
       " ('big business', 18),\n",
       " ('not only', 18),\n",
       " ('for years', 18),\n",
       " ('all of', 18),\n",
       " ('you think', 17),\n",
       " ('i want', 17),\n",
       " ('have you', 17),\n",
       " ('you mentioned', 17),\n",
       " ('when you', 17),\n",
       " ('your second', 17),\n",
       " ('if someone', 17),\n",
       " ('as an', 17),\n",
       " ('we have', 17),\n",
       " ('none of', 17),\n",
       " ('it should', 16),\n",
       " ('think of', 16),\n",
       " ('i certainly', 16),\n",
       " ('if it', 16),\n",
       " ('my only', 16),\n",
       " ('not really', 16),\n",
       " ('people who', 16),\n",
       " ('in general', 16),\n",
       " ('the key', 16),\n",
       " ('i always', 16),\n",
       " ('you need', 16),\n",
       " ('i dont', 16),\n",
       " ('you realize', 16),\n",
       " ('my question', 16),\n",
       " ('if ,', 16),\n",
       " ('it sucks', 16),\n",
       " ('you would', 15),\n",
       " ('this sounds', 15),\n",
       " ('it just', 15),\n",
       " ('my understanding', 14),\n",
       " ('in some', 14),\n",
       " ('in a', 14),\n",
       " ('you raise', 14),\n",
       " ('one could', 14),\n",
       " ('developed countries', 14),\n",
       " ('in terms', 14),\n",
       " ('you seem', 14),\n",
       " ('in all', 14),\n",
       " ('my original', 14),\n",
       " ('this in', 14),\n",
       " ('terrifyingly effective', 14),\n",
       " ('i honestly', 13),\n",
       " ('i probably', 13),\n",
       " ('the first', 13),\n",
       " ('i get', 13),\n",
       " ('not necessarily', 13),\n",
       " ('looking at', 13),\n",
       " ('there has', 13),\n",
       " ('we should', 13),\n",
       " ('it can', 13),\n",
       " ('you changed', 13),\n",
       " ('the question', 13),\n",
       " ('you must', 13),\n",
       " ('this should', 12),\n",
       " ('whenever i', 12),\n",
       " ('is not', 12),\n",
       " ('this image', 12),\n",
       " ('illegal immigrants', 12),\n",
       " ('you mean', 12),\n",
       " ('you said', 12),\n",
       " ('my main', 12),\n",
       " ('in that', 12),\n",
       " ('in 2008', 12),\n",
       " ('suicide is', 12),\n",
       " ('it might', 12),\n",
       " ('world war', 12),\n",
       " ('i mentioned', 12),\n",
       " ('on a', 12),\n",
       " ('it probably', 12),\n",
       " ('what struck', 12),\n",
       " ('they can', 11),\n",
       " ('here are', 11),\n",
       " ('is it', 11),\n",
       " ('after reading', 11),\n",
       " ('your first', 11),\n",
       " ('with the', 11),\n",
       " (\"we '\", 11),\n",
       " ('this article', 11),\n",
       " ('you bring', 11),\n",
       " ('a better', 11),\n",
       " ('i mostly', 11),\n",
       " ('the underlying', 11),\n",
       " ('as far', 11),\n",
       " ('you made', 11),\n",
       " ('this seems', 11),\n",
       " ('why should', 11),\n",
       " ('i used', 11),\n",
       " ('all three', 11),\n",
       " ('in football', 11),\n",
       " ('i thought', 10),\n",
       " ('the best', 10),\n",
       " ('the recent', 10),\n",
       " ('based on', 10),\n",
       " ('to say', 10),\n",
       " ('the bottom', 10),\n",
       " ('the other', 10),\n",
       " ('can i', 10),\n",
       " ('civilians who', 10),\n",
       " ('we all', 10),\n",
       " ('people have', 10),\n",
       " ('my issue', 10),\n",
       " ('going back', 10),\n",
       " ('we agree', 10),\n",
       " ('i made', 10),\n",
       " ('everyone around', 10),\n",
       " ('in december', 10),\n",
       " ('it matters', 10),\n",
       " ('the title', 10),\n",
       " ('not just', 10),\n",
       " ('my basic', 10),\n",
       " ('which is', 10),\n",
       " ('you even', 10),\n",
       " ('cheese sustains', 10),\n",
       " ('as of', 10),\n",
       " ('when that', 10),\n",
       " ('existential inertia', 10),\n",
       " ('when we', 10),\n",
       " ('i totally', 10),\n",
       " ('there also', 10),\n",
       " ('for the', 10),\n",
       " ('the flaw', 10),\n",
       " ('i said', 10),\n",
       " ('this meant', 10),\n",
       " ('master in', 10),\n",
       " ('the legal', 10),\n",
       " ('you believe', 9),\n",
       " ('this argument', 9),\n",
       " ('it all', 9),\n",
       " ('it makes', 9),\n",
       " ('i generally', 9),\n",
       " ('these two', 9),\n",
       " ('you know', 9),\n",
       " ('your last', 9),\n",
       " ('why is', 9),\n",
       " ('this goes', 9),\n",
       " ('this also', 9),\n",
       " ('this point', 9),\n",
       " ('not all', 9),\n",
       " ('the final', 9),\n",
       " ('i love', 9),\n",
       " ('i appreciate', 9),\n",
       " ('you just', 9),\n",
       " ('the view', 9),\n",
       " ('the thought', 9),\n",
       " ('when people', 9),\n",
       " ('when a', 9),\n",
       " ('i tried', 9),\n",
       " ('i recently', 9),\n",
       " ('who cares', 9),\n",
       " ('this ,', 9),\n",
       " ('the article', 9),\n",
       " ('not true', 9),\n",
       " ('i genuinely', 9),\n",
       " ('would stop', 9),\n",
       " ('an argument', 8),\n",
       " ('the same', 8),\n",
       " ('the truth', 8),\n",
       " ('in both', 8),\n",
       " ('why are', 8),\n",
       " ('the morality', 8),\n",
       " (\"would '\", 8),\n",
       " ('i live', 8),\n",
       " ('the draft', 8),\n",
       " ('it goes', 8),\n",
       " ('the common', 8),\n",
       " ('consensus is', 8),\n",
       " ('this leads', 8),\n",
       " ('you understand', 8),\n",
       " ('this thread', 8),\n",
       " ('this might', 8),\n",
       " ('divorce can', 8),\n",
       " ('they were', 8),\n",
       " ('who does', 8),\n",
       " ('in situation', 8),\n",
       " ('being fat', 8),\n",
       " ('using your', 8),\n",
       " ('racist should', 8),\n",
       " ('it appears', 8),\n",
       " ('the two', 8),\n",
       " ('even then', 8),\n",
       " ('children do', 8),\n",
       " ('i digress', 8),\n",
       " ('this website', 8),\n",
       " ('open carrying', 8),\n",
       " ('that means', 8),\n",
       " ('statutes of', 8),\n",
       " ('the analogy', 8),\n",
       " ('the original', 8),\n",
       " ('the oxford', 8),\n",
       " ('i know', 8),\n",
       " ('the hypocrisy', 8),\n",
       " ('net neutrality', 8),\n",
       " ('i sort', 8),\n",
       " ('my answer', 8),\n",
       " ('you assume', 8),\n",
       " ('the claim', 7),\n",
       " ('one can', 7),\n",
       " ('the reality', 7),\n",
       " ('you and', 7),\n",
       " ('until recently', 7),\n",
       " ('you definitely', 7),\n",
       " ('people on', 7),\n",
       " ('by a', 7),\n",
       " ('you get', 7),\n",
       " ('by this', 7),\n",
       " ('take a', 7),\n",
       " ('i stand', 7),\n",
       " ('the word', 7),\n",
       " ('i truly', 7),\n",
       " ('after watching', 7),\n",
       " ('in any', 7),\n",
       " ('my wife', 7),\n",
       " ('we live', 7),\n",
       " ('they would', 7),\n",
       " ('the vast', 7),\n",
       " ('everyone is', 7),\n",
       " ('when considering', 7),\n",
       " ('this (', 7),\n",
       " ('if there', 7),\n",
       " ('take the', 7),\n",
       " ('the above', 7),\n",
       " ('what you', 7),\n",
       " ('your main', 7),\n",
       " ('i very', 7),\n",
       " ('some countries', 7),\n",
       " ('smoking during', 7),\n",
       " ('many of', 7),\n",
       " ('it turns', 7),\n",
       " ('people claim', 7),\n",
       " ('nobody makes', 7),\n",
       " ('it says', 7),\n",
       " ('we do', 7),\n",
       " ('no swimmer', 6),\n",
       " ('theft was', 6),\n",
       " ('with that', 6),\n",
       " ('the academics', 6),\n",
       " ('if one', 6),\n",
       " ('the obvious', 6),\n",
       " ('the man', 6),\n",
       " ('one issue', 6),\n",
       " ('are there', 6),\n",
       " ('this makes', 6),\n",
       " ('this statement', 6),\n",
       " ('essential oils', 6),\n",
       " ('i went', 6),\n",
       " ('animal products', 6),\n",
       " ('what happens', 6),\n",
       " ('whenever futuristic', 6),\n",
       " ('all americans', 6),\n",
       " ('black nationalism', 6),\n",
       " ('one thing', 6),\n",
       " ('according to', 6),\n",
       " ('thats ridiculous', 6),\n",
       " ('i gave', 6),\n",
       " ('grading bias', 6),\n",
       " ('the more', 6),\n",
       " ('does it', 6),\n",
       " ('my girlfriend', 6),\n",
       " ('someone like', 6),\n",
       " ('you mention', 6),\n",
       " ('the whole', 6),\n",
       " ('to be', 6),\n",
       " ('the debate', 6),\n",
       " ('most women', 6),\n",
       " ('everyone thinking', 6),\n",
       " ('can anyone', 6),\n",
       " ('i absolutely', 6),\n",
       " ('this study', 6),\n",
       " ('tablets are', 6),\n",
       " ('it did', 6),\n",
       " ('this stems', 6),\n",
       " ('you agree', 6),\n",
       " ('drawing a', 6),\n",
       " ('your post', 6),\n",
       " ('civil war', 6),\n",
       " ('why am', 6),\n",
       " ('in no', 6),\n",
       " ('your cmv', 6),\n",
       " ('california has', 6),\n",
       " ('i firmly', 6),\n",
       " ('i take', 6),\n",
       " ('people around', 6),\n",
       " ('i ask', 6),\n",
       " ('an adult', 6),\n",
       " ('retaliatory behavior', 6),\n",
       " ('the little', 6),\n",
       " ('the million', 6),\n",
       " ('after having', 6),\n",
       " ('my piece', 6),\n",
       " ('to argue', 6),\n",
       " ('my final', 6),\n",
       " ('since then', 6),\n",
       " ('before 2011', 6),\n",
       " ('teacher is', 6),\n",
       " ('i agreed', 6),\n",
       " ('in college', 6),\n",
       " ('these concepts', 6),\n",
       " ('he was', 6),\n",
       " ('this concludes', 6),\n",
       " ('over the', 6),\n",
       " ('religion has', 6),\n",
       " ('in reality', 6),\n",
       " ('one would', 6),\n",
       " ('a typical', 6),\n",
       " ('just read', 6),\n",
       " ('let us', 6),\n",
       " ('economic policies', 6),\n",
       " (\"america '\", 6),\n",
       " ('each individual', 6),\n",
       " ('this double', 6),\n",
       " ('hard work', 6),\n",
       " ('this works', 6),\n",
       " ('some companies', 6),\n",
       " ('we uphold', 6),\n",
       " ('not always', 6),\n",
       " ('wikipedia first', 6),\n",
       " ('i admit', 6),\n",
       " ('watching the', 6),\n",
       " ('prior to', 6),\n",
       " ('it solidifies', 6),\n",
       " ('to compare', 6),\n",
       " ('this probably', 6),\n",
       " ('cheating happens', 6),\n",
       " ('i no', 6),\n",
       " ('the conversation', 6),\n",
       " ('the origin', 6),\n",
       " ('if all', 5),\n",
       " ('to claim', 5),\n",
       " ('the cost', 5),\n",
       " ('a common', 5),\n",
       " ('in canada', 5),\n",
       " ('in cases', 5),\n",
       " ('thinking about', 5),\n",
       " ('my whole', 5),\n",
       " ('people might', 5),\n",
       " ('i realized', 5),\n",
       " ('this would', 5),\n",
       " ('nothing ,', 5),\n",
       " ('the last', 5),\n",
       " ('gay marriage', 5),\n",
       " (\"they '\", 5),\n",
       " ('those who', 5),\n",
       " ('my logic', 5),\n",
       " ('he is', 5),\n",
       " ('i originally', 5),\n",
       " ('this whole', 5),\n",
       " ('apart from', 5),\n",
       " ('the purpose', 5),\n",
       " ('i strongly', 5),\n",
       " ('the short', 5),\n",
       " ('you keep', 5),\n",
       " ('in practice', 5),\n",
       " ('in many', 5),\n",
       " ('the \"', 5),\n",
       " ('does not', 5),\n",
       " ('whatever is', 5),\n",
       " ('it means', 5),\n",
       " (\"the '\", 5),\n",
       " ('one problem', 5),\n",
       " ('where i', 5),\n",
       " ('if people', 5),\n",
       " ('i got', 5),\n",
       " ('not every', 5),\n",
       " ('your original', 5),\n",
       " ('it depends', 5),\n",
       " ('i believe', 5),\n",
       " ('studies show', 5),\n",
       " ('the media', 5),\n",
       " ('the fundamental', 5),\n",
       " ('some do', 5),\n",
       " ('i (', 5),\n",
       " ('why do', 5),\n",
       " ('you asked', 5),\n",
       " ('i pointed', 5),\n",
       " ('other people', 5),\n",
       " ('you acknowledge', 5),\n",
       " ('a growing', 5),\n",
       " ('people are', 5),\n",
       " ('i wonder', 5),\n",
       " ('i decided', 5),\n",
       " ('the premise', 5),\n",
       " ('you brought', 5),\n",
       " ('murderers exist', 5),\n",
       " ('i hold', 5),\n",
       " ('you misunderstand', 5),\n",
       " (\"who '\", 5),\n",
       " ('a very', 5),\n",
       " ('you want', 5),\n",
       " ('have a', 5),\n",
       " ('video games', 5),\n",
       " ('you were', 5),\n",
       " ('this idea', 5),\n",
       " ('the latter', 5),\n",
       " ('to use', 5),\n",
       " ('my so', 5),\n",
       " ('someone else', 5),\n",
       " ('i eat', 5),\n",
       " ('when it', 5),\n",
       " ('i need', 4),\n",
       " ('after seeing', 4),\n",
       " ('the governor', 4),\n",
       " ('it still', 4),\n",
       " ('a great', 4),\n",
       " ('to address', 4),\n",
       " ('i awarded', 4),\n",
       " ('being a', 4),\n",
       " ('the biggest', 4),\n",
       " ('how many', 4),\n",
       " ('sweat shops', 4),\n",
       " ('you neglect', 4),\n",
       " ('religion is', 4),\n",
       " ('both genres', 4),\n",
       " ('both contain', 4),\n",
       " ('the proton', 4),\n",
       " ('i hate', 4),\n",
       " ('i sometimes', 4),\n",
       " ('you first', 4),\n",
       " ('some exapnding', 4),\n",
       " ('the quran', 4),\n",
       " ('the news', 4),\n",
       " ('the efficiency', 4),\n",
       " ('to give', 4),\n",
       " ('this proves', 4),\n",
       " ('you will', 4),\n",
       " ('some credit', 4),\n",
       " ('i disobey', 4),\n",
       " ('this all', 4),\n",
       " ('rape cases', 4),\n",
       " ('rock beat', 4),\n",
       " ('you ,', 4),\n",
       " ('as for', 4),\n",
       " ('many are', 4),\n",
       " ('nuclear weapons', 4),\n",
       " ('this came', 4),\n",
       " ('i recognize', 4),\n",
       " ('its not', 4),\n",
       " ('take for', 4),\n",
       " ('social justice', 4),\n",
       " ('muslims -', 4),\n",
       " ('the conclusion', 4),\n",
       " ('you wont', 4),\n",
       " ('this gets', 4),\n",
       " ('if they', 4),\n",
       " ('a counter', 4),\n",
       " ('hate crimes', 4),\n",
       " ('these things', 4),\n",
       " ('i forgot', 4),\n",
       " ('today was', 4),\n",
       " ('stan lee', 4),\n",
       " ('the numbers', 4),\n",
       " ('the distinction', 4),\n",
       " ('i give', 4),\n",
       " ('tesla was', 4),\n",
       " ('nothing should', 4),\n",
       " ('heating skill', 4),\n",
       " ('the is', 4),\n",
       " ('from my', 4),\n",
       " ('over time', 4),\n",
       " ('i accept', 4),\n",
       " ('this does', 4),\n",
       " ('we ended', 4),\n",
       " ('if not', 4),\n",
       " ('the soldier', 4),\n",
       " ('adults i', 4),\n",
       " ('this leaves', 4),\n",
       " ('the post', 4),\n",
       " ('did you', 4),\n",
       " ('if anyone', 4),\n",
       " ('i found', 4),\n",
       " ('whenever an', 4),\n",
       " ('i care', 4),\n",
       " ('her argument', 4),\n",
       " ('the line', 4),\n",
       " ('every attribute', 4),\n",
       " ('once sliced', 4),\n",
       " ('the pay', 4),\n",
       " ('semantics matter', 4),\n",
       " ('in order', 4),\n",
       " ('reading through', 4),\n",
       " ('with all', 4),\n",
       " ('background checks', 4),\n",
       " ('creating a', 4),\n",
       " ('these days', 4),\n",
       " ('when talking', 4),\n",
       " ('alola forms', 4),\n",
       " ('all languages', 4),\n",
       " ('the theory', 4),\n",
       " ('critics say', 4),\n",
       " ('i disaprove', 4),\n",
       " ('stubborn might', 4),\n",
       " ('at some', 4),\n",
       " ('quite inflammatory', 4),\n",
       " ('given (', 4),\n",
       " ('personal love', 4),\n",
       " ('if trump', 4),\n",
       " ('we both', 4),\n",
       " ('language evolves', 4),\n",
       " ('after gay', 4),\n",
       " ('my personal', 4),\n",
       " ('having seen', 4),\n",
       " ('if those', 4),\n",
       " ('is your', 4),\n",
       " ('anyone i', 4),\n",
       " ('girls outperform', 4),\n",
       " ('the majority', 4),\n",
       " ('global warming', 4),\n",
       " ('the bible', 4),\n",
       " ('the are', 4),\n",
       " ('the united', 4),\n",
       " ('drug proponents', 4),\n",
       " ('sun and', 4),\n",
       " ('this changes', 4),\n",
       " ('assuming your', 4),\n",
       " ('you tell', 4),\n",
       " ('time ,', 4),\n",
       " ('there might', 4),\n",
       " ('the primary', 4),\n",
       " ('that shocked', 4),\n",
       " ('you wrote', 4),\n",
       " ('\" planet', 4),\n",
       " ('my comment', 4),\n",
       " ('healthcare :', 4),\n",
       " ('this topic', 4),\n",
       " ('bombers were', 4),\n",
       " ('prostitution is', 4),\n",
       " ('big government', 4),\n",
       " ('we must', 4),\n",
       " ('having read', 4),\n",
       " ('both versions', 4),\n",
       " ('both of', 4),\n",
       " ('non -', 4),\n",
       " (\"y '\", 4),\n",
       " ('the case', 4),\n",
       " ('please quote', 4),\n",
       " ('profit =', 4),\n",
       " ('by worldview', 4),\n",
       " ('my cmv', 4),\n",
       " ('my title', 4),\n",
       " ('life in', 4),\n",
       " ('those people', 4),\n",
       " ('i award', 4),\n",
       " ('flying is', 4),\n",
       " ('the notion', 4),\n",
       " ('by your', 4),\n",
       " ('i reject', 4),\n",
       " ('porn has', 4),\n",
       " ('you recognize', 4),\n",
       " ('people bash', 4),\n",
       " ('there already', 4),\n",
       " ('the dictionary', 4),\n",
       " ('this already', 4),\n",
       " ('i argue', 4),\n",
       " ('polar bears', 4),\n",
       " ('the contention', 4),\n",
       " ('the principle', 4),\n",
       " ('corporate personhood', 4),\n",
       " ('i misspoke', 4),\n",
       " ('low scoring', 4),\n",
       " ('every cent', 4),\n",
       " ('a more', 4),\n",
       " ('it naturally', 4),\n",
       " ('i specifically', 4),\n",
       " ('the former', 4),\n",
       " ('in recent', 4),\n",
       " ('that all', 4),\n",
       " ('going beyond', 4),\n",
       " ('they did', 4),\n",
       " ('they have', 4),\n",
       " ('i come', 4),\n",
       " ('the example', 4),\n",
       " ('the average', 4),\n",
       " ('readers enjoy', 4),\n",
       " ('100 ,', 4),\n",
       " ('some context', 4),\n",
       " ('research shows', 4),\n",
       " ('my quandary', 4),\n",
       " ('lately (', 4),\n",
       " ('the crew', 4),\n",
       " ('i partially', 4),\n",
       " ('part of', 4),\n",
       " ('batman is', 4),\n",
       " ('i asked', 4),\n",
       " ('some control', 4),\n",
       " ('they could', 4),\n",
       " ('why stop', 4),\n",
       " ('it works', 4),\n",
       " ('the logical', 4),\n",
       " ('i dispute', 4),\n",
       " ('why touche', 4),\n",
       " ('my fundamental', 4),\n",
       " ('the probability', 4),\n",
       " ('there should', 4),\n",
       " ('i try', 4),\n",
       " ('working outside', 3),\n",
       " ('i knew', 3),\n",
       " ('the very', 3),\n",
       " ('you pointed', 3),\n",
       " ('a good', 3),\n",
       " ('the rationale', 3),\n",
       " ('the important', 3),\n",
       " ('coming back', 3),\n",
       " ('the generally', 3),\n",
       " ('it really', 3),\n",
       " ('the current', 3),\n",
       " ('in 2014', 3),\n",
       " ('this directly', 3),\n",
       " ('not op', 3),\n",
       " ('its also', 3),\n",
       " ('it comes', 3),\n",
       " ('since 1997', 3),\n",
       " ('if anything', 3),\n",
       " ('this explains', 3),\n",
       " ('just the', 3),\n",
       " ('as i', 3),\n",
       " ('without even', 3),\n",
       " ('why then', 3),\n",
       " ('by saying', 3),\n",
       " ('yours was', 3),\n",
       " ('seatbelts ?', 3),\n",
       " ('the correct', 3),\n",
       " ('ignoring the', 3),\n",
       " ('this line', 3),\n",
       " ('in their', 3),\n",
       " ('you stated', 3),\n",
       " ('your assumption', 3),\n",
       " ('your points', 3),\n",
       " ('in game', 3),\n",
       " ('mr .', 3),\n",
       " ('my larger', 3),\n",
       " ('the military', 3),\n",
       " ('the one', 3),\n",
       " ('that also', 3),\n",
       " ('past elements', 3),\n",
       " ('lets not', 3),\n",
       " ('i simply', 3),\n",
       " ('for some', 3),\n",
       " ('the evidence', 3),\n",
       " ('person a', 3),\n",
       " ('the cheesesteak', 3),\n",
       " ('to go', 3),\n",
       " ('one common', 3),\n",
       " ('it essentially', 3),\n",
       " ('this issue', 3),\n",
       " ('those numbers', 3),\n",
       " ('your other', 3),\n",
       " ('the history', 3),\n",
       " ('do they', 3),\n",
       " ('everyone has', 3),\n",
       " ('you hit', 3),\n",
       " ('there seems', 3),\n",
       " ('every person', 3),\n",
       " ('every possibility', 3),\n",
       " ('pointing out', 3),\n",
       " ('you deserve', 3),\n",
       " ('i ,', 3),\n",
       " ('one reason', 3),\n",
       " ('the court', 3),\n",
       " ('on mobile', 3),\n",
       " ('i apologise', 3),\n",
       " ('one final', 3),\n",
       " ('consider the', 3),\n",
       " ('through all', 3),\n",
       " ('religious institutions', 3),\n",
       " ('is this', 3),\n",
       " ('just one', 3),\n",
       " ('people say', 3),\n",
       " ('the situation', 3),\n",
       " ('on the', 3),\n",
       " ('it isnt', 3),\n",
       " ('all the', 3),\n",
       " ('why warfighter', 3),\n",
       " ('the entire', 3),\n",
       " ('probably not', 3),\n",
       " ('third wave', 3),\n",
       " ('which films', 3),\n",
       " ('there certainly', 3),\n",
       " ('this brings', 3),\n",
       " ('the mother', 3),\n",
       " ('your title', 3),\n",
       " ('financial harm', 3),\n",
       " ('people can', 3),\n",
       " ('how come', 3),\n",
       " ('this can', 3),\n",
       " ('lets look', 3),\n",
       " ('your \"', 3),\n",
       " ('they wont', 3),\n",
       " ('how do', 3),\n",
       " ('your position', 3),\n",
       " ('your implication', 3),\n",
       " ('you presuppose', 3),\n",
       " ('the way', 3),\n",
       " ('the good', 3),\n",
       " ('the us', 3),\n",
       " ('they should', 3),\n",
       " ('what got', 3),\n",
       " ('can we', 3),\n",
       " ('the quote', 3),\n",
       " ('it only', 3),\n",
       " ('in someways', 3),\n",
       " ('the real', 3),\n",
       " ('this post', 3),\n",
       " ('who said', 3),\n",
       " ('the structure', 3),\n",
       " ('for me', 3),\n",
       " ('your comment', 3),\n",
       " ('it nowhere', 3),\n",
       " ('queer \"', 3),\n",
       " ('you better', 3),\n",
       " ('this again', 3),\n",
       " ('100 %', 3),\n",
       " ('financial stability', 3),\n",
       " ('i apologize', 3),\n",
       " ('is that', 3),\n",
       " ('every time', 3),\n",
       " ('the results', 3),\n",
       " ('health risks', 3),\n",
       " ('the unfortunate', 3),\n",
       " ('a public', 3),\n",
       " ('different people', 3),\n",
       " ('environmental damage', 3),\n",
       " ('health effects', 3),\n",
       " ('with your', 3),\n",
       " ('the crux', 3),\n",
       " ('the supreme', 3),\n",
       " ('an equally', 3),\n",
       " ('it so', 3),\n",
       " ('this kind', 3),\n",
       " ('for a', 3),\n",
       " ('the dilemma', 3),\n",
       " ('any of', 3),\n",
       " ('by that', 3),\n",
       " ('do not', 3),\n",
       " ('that sucks', 3),\n",
       " ('a less', 3),\n",
       " ('i find', 3),\n",
       " ('it happens', 3),\n",
       " ('having established', 3),\n",
       " ('your opinion', 3),\n",
       " ('how so', 3),\n",
       " ('my problem', 3),\n",
       " ('i make', 3),\n",
       " ('i brought', 3),\n",
       " ('can it', 3),\n",
       " ('the process', 3),\n",
       " ('my anecdotal', 3),\n",
       " ('at its', 3),\n",
       " ('why can', 3),\n",
       " ('is a', 3),\n",
       " ('your claim', 3),\n",
       " ('it saddens', 3),\n",
       " ('the term', 3),\n",
       " ('in theory', 3),\n",
       " ('the fermi', 3),\n",
       " ('i ignored', 3),\n",
       " ('you all', 3),\n",
       " ('you give', 3),\n",
       " ('i worded', 3),\n",
       " ('you forget', 3),\n",
       " ('makeup is', 3),\n",
       " ('how am', 3),\n",
       " ('there may', 3),\n",
       " ('at risk', 3),\n",
       " ('lets go', 3),\n",
       " ('i acknowledge', 3),\n",
       " ('even one', 3),\n",
       " ('what the', 3),\n",
       " ('debating religion', 3),\n",
       " ('an institution', 3),\n",
       " ('the catholic', 3),\n",
       " ('you actually', 3),\n",
       " ('ignoring for', 3),\n",
       " ('why the', 3),\n",
       " ('is incorrect', 3),\n",
       " ('no pain', 3),\n",
       " ('each case', 3),\n",
       " ('i world', 3),\n",
       " ('you still', 3),\n",
       " ('my opinion', 3),\n",
       " ('one (', 3),\n",
       " ('this source', 3),\n",
       " ('that ad', 2),\n",
       " ('other folks', 2),\n",
       " ('the third', 2),\n",
       " ('we also', 2),\n",
       " ('compare all', 2),\n",
       " ('biological bonds', 2),\n",
       " ('who says', 2),\n",
       " ('guns are', 2),\n",
       " ('you view', 2),\n",
       " ('why would', 2),\n",
       " ('nuclear monopoly', 2),\n",
       " ('the racism', 2),\n",
       " ('why university', 2),\n",
       " ('both sides', 2),\n",
       " ('amidst all', 2),\n",
       " ('last year', 2),\n",
       " ('restrooms separated', 2),\n",
       " ('hardship hardship', 2),\n",
       " ('it posits', 2),\n",
       " ('as men', 2),\n",
       " ('why so', 2),\n",
       " ('my ultimate', 2),\n",
       " ('people become', 2),\n",
       " ('parts of', 2),\n",
       " ('i stop', 2),\n",
       " ('increased cost', 2),\n",
       " ('for genderfluid', 2),\n",
       " ('if he', 2),\n",
       " ('no vote', 2),\n",
       " ('you didnt', 2),\n",
       " ('watch this', 2),\n",
       " ('regarding immigration', 2),\n",
       " ('people tend', 2),\n",
       " ('this sort', 2),\n",
       " ('to deny', 2),\n",
       " ('the name', 2),\n",
       " ('self defence', 2),\n",
       " ('from europe', 2),\n",
       " ('to suggest', 2),\n",
       " ('intrest rates', 2),\n",
       " ('one student', 2),\n",
       " ('this delta', 2),\n",
       " ('your basic', 2),\n",
       " ('wearing more', 2),\n",
       " ('without the', 2),\n",
       " ('you missed', 2),\n",
       " ('the guy', 2),\n",
       " ('definitely not', 2),\n",
       " ('i put', 2),\n",
       " ('after thinking', 2),\n",
       " ('the djia', 2),\n",
       " ('to cmv', 2),\n",
       " ('the result', 2),\n",
       " ('if reddit', 2),\n",
       " ('people (', 2),\n",
       " ('critical evaluations', 2),\n",
       " ('a2 is', 2),\n",
       " ('another kind', 2),\n",
       " ('the concern', 2),\n",
       " ('repetitive behavior', 2),\n",
       " ('my bs', 2),\n",
       " ('the nfl', 2),\n",
       " ('the republican', 2),\n",
       " ('free will', 2),\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the clean post data and prepare paragraph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_posts = [[], []]\n",
    "for i, key in enumerate(['pos', 'neg']):\n",
    "    with open('./../preprocess/cmv_raw/parsing/clean_{}_single'.format(key)) as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            clean_posts[i].append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [{}, {}]\n",
    "for side in range(2):\n",
    "    for clean_post in clean_posts[side]:\n",
    "        topic_id, post_id = [int(_) for _ in clean_post['uid'].split('_')]\n",
    "\n",
    "        # if there are post longer than 480\n",
    "        if(topic_id in check):\n",
    "            continue\n",
    "\n",
    "        if(topic_id not in posts[side]):\n",
    "            posts[side][topic_id] = {\n",
    "                'topic':clean_post['topic'],\n",
    "                'content':[]\n",
    "            }\n",
    "        while(len(posts[side][topic_id]['content'])<=post_id):\n",
    "            posts[side][topic_id]['content'].append(None)\n",
    "            \n",
    "        posts[side][topic_id]['content'][post_id] = {\n",
    "            'bio':[None for _ in clean_post['context']],\n",
    "            'context':clean_post['context'],\n",
    "            'type':[None for _ in clean_post['context']]\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for side in range(2):\n",
    "    for index, sent, bio, dtype in zip(recover[side]['index'], recover[side]['sent'], recover[side]['bio'], recover[side]['type']):\n",
    "        topic_id, post_id, para_id = [ int(_) for _ in index_mapping[side][index].split('_')]\n",
    "        \n",
    "        if(topic_id not in posts[side]):\n",
    "            continue\n",
    "            \n",
    "        posts[side][topic_id]['content'][post_id]['context'][para_id] = sent\n",
    "        posts[side][topic_id]['content'][post_id]['bio'][para_id] = ' '.join(bio)\n",
    "        posts[side][topic_id]['content'][post_id]['type'][para_id] = dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for side in range(2):\n",
    "    topic_ids = list(posts[side].keys())\n",
    "    \n",
    "    for topic_id in topic_ids:\n",
    "        dtype = []\n",
    "        try:\n",
    "            for post_id in range(len( posts[side][topic_id]['content'] )):\n",
    "                dtype = []\n",
    "                for _ in posts[side][topic_id]['content'][post_id]['type']:\n",
    "                    dtype.extend(_)\n",
    "                posts[side][topic_id]['content'][post_id]['type'] = dtype\n",
    "        except TypeError:\n",
    "            for side in range(2):\n",
    "                if(topic_id in posts[side]):\n",
    "                    del posts[side][topic_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bio': ['O O O O O B I I I I I I I I I I I I I I I O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I O B I I I I I I I I I I I I I I I I I I I I I I I O',\n",
       "  'O O O O O O B I I I I I I O O O O O O O O O O O O O',\n",
       "  'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B I I I I I I I I O O B I I I I I I I I I I I I I I I I I I I O O O O O B I I I I I I I O O B I I I I I I I I I I I I I I I I I I I I O O O O O O O O O O O O O O O O O O O O'],\n",
       " 'context': [\"i have to say that i am very disappointed with the current state of political discourse in today ' s society . both in mass media and the internet , political discussion seems to be ruled by angry extremists who think that the other side is evil and shout down , insult and in some case censor anyone who does not think so . many times over i ' ve dealt with those types of people ( both from the left and the right ) only to find out that reason and logic rarely , if ever work on them . their extreme views also focus a lot of outrage ( sometimes from other extremists ) , often derailing the discussion from the original topic .\",\n",
       "  'so i have a question : do you guys this can be fixed ? or is this just the gift in practise ? ( link )',\n",
       "  \"> hello , users of cmv ! this is a footnote from your moderators . we ' d just like to remind you of a couple of things . firstly , please remember to read through our rules ( link ) . if you see a comment that has broken one , it is more effective to report it than downvote it . speaking of which , downvotes do ' t change views ( link ) ! if you are thinking about submitting a cmv yourself , please have a look through our popular topics wiki ( link ) first . any questions or concerns ? feel free to message us ( link ) . happy cmving !\"],\n",
       " 'type': ['C', 'P', 'P', 'P', 'C', 'P', 'P', 'P', 'P']}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[0][0]['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process link \n",
    "EOS_tokens_list = [\".\", \"!\", \"?\", \"</ac>\", \"<para>\"]\n",
    "\n",
    "def find_shell(index, last, context, adu_index):\n",
    "    if(index==0):\n",
    "        return (0, 0)\n",
    "    for i in range(index, last-1, -1):\n",
    "        if(context[i] in EOS_tokens_list):\n",
    "            break\n",
    "    return (adu_index, i , index-1)\n",
    "\n",
    "def prepare(data, elmo_preprocess):\n",
    "    mask = []\n",
    "    shell_span, span, elmo_index = [], [], []\n",
    "    adu_index = 0\n",
    "    adu_label = []\n",
    "    \n",
    "    topic_elmo_index = len(elmo_preprocess)\n",
    "    elmo_preprocess.append('<topic> '+data['topic']+' </topic>')\n",
    "    elmo_preprocess[-1] = ' '.join(elmo_preprocess[-1].split())\n",
    "    \n",
    "    ac_position_info = []\n",
    "    \n",
    "    for post_pos, post in enumerate(data['content']):\n",
    "        for bio, context in zip(post['bio'], post['context']):\n",
    "            elmo_index.append(len(elmo_preprocess))\n",
    "\n",
    "            elmo_preprocess.append(['<para>'])\n",
    "            bio, context = bio.split(), context.split()\n",
    "\n",
    "            last = None\n",
    "            for l, word in zip(bio, context):\n",
    "                if((l=='O' or l=='B') and last=='I'):\n",
    "                    elmo_preprocess[-1].append('</ac>')\n",
    "                if(l=='B'):\n",
    "                    elmo_preprocess[-1].append('<ac>')\n",
    "                elmo_preprocess[-1].append(word)\n",
    "\n",
    "                last = l\n",
    "\n",
    "            elmo_preprocess[-1].append('</para>')\n",
    "\n",
    "            last = -1\n",
    "            for index in range(len(elmo_preprocess[-1])):\n",
    "                if(elmo_preprocess[-1][index] == '<ac>'):\n",
    "                    span.append([adu_index, index, 0])\n",
    "                    shell_span.append(find_shell(index, last, elmo_preprocess[-1], adu_index))\n",
    "\n",
    "                elif(elmo_preprocess[-1][index] == '</ac>'):\n",
    "                    assert(span[-1][-1]==0), \"last should be zero\"\n",
    "                    span[-1][-1] = index\n",
    "                    last = index\n",
    "            elmo_preprocess[-1] = ' '.join(elmo_preprocess[-1])\n",
    "            # update parameter\n",
    "            adu_index += 1\n",
    "\n",
    "        # build mask\n",
    "        dtype = post['type']\n",
    "        prev_len, now_len = len(adu_label), len(dtype)\n",
    "        for adu_pos, _ in enumerate(dtype):\n",
    "            if(_ == 'P'):\n",
    "                # constrain search space to this reply\n",
    "                mask.append([0]*prev_len + [1]*now_len)\n",
    "\n",
    "            elif(_ == 'C'):\n",
    "                # constrain search space to all\n",
    "                mask.append([1]*(prev_len + now_len))\n",
    "\n",
    "            ac_position_info.append([adu_pos, post_pos])\n",
    "\n",
    "        adu_label.extend( dtype )\n",
    "\n",
    "        \n",
    "        \n",
    "    return {\n",
    "        'topic_index':topic_elmo_index, \n",
    "        'elmo_index':elmo_index,\n",
    "        'shell_span':shell_span, \n",
    "        'span':span, \n",
    "        'mask':mask, \n",
    "        'adu_label':adu_label,\n",
    "        'ac_position_info':ac_position_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "last should be zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-23c959e776d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mside\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtopic_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melmo_preprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-59ec87ab16b2>\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(data, elmo_preprocess)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melmo_preprocess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'</ac>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last should be zero\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                     \u001b[0mspan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: last should be zero"
     ]
    }
   ],
   "source": [
    "prepare_data = [[], []]\n",
    "elmo_preprocess = []\n",
    "\n",
    "for side in range(2):\n",
    "    for topic_id in posts[side]:\n",
    "        temp = prepare(posts[side][topic_id], elmo_preprocess)\n",
    "        temp['uid'] = '{}_{}'.format(side, topic_id)\n",
    "        \n",
    "        prepare_data[side].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['topic_index', 'elmo_index', 'shell_span', 'span', 'mask', 'adu_label', 'ac_position_info', 'uid'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_data[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "54\n",
      "68\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "for side in range(2):\n",
    "    for _ in prepare_data[side]:\n",
    "        m = max(m, len(_['span']))\n",
    "        if(m == len(_['span'])):\n",
    "            print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<para> <ac> what i think we should do is go after these people </ac> . <ac> if instead of a giant wall and an insane increase in a task force , it would be a lot cheaper to have a regulative authority that regularly audits and investigates companies that engage in this practice </ac> . and <ac> companies that are found engaging in it should face severe punishment </ac> ... .i 'm talking massive fines for first offense and revoking of business licenses in other offenses , potentially increasing to the point of confiscating the business and property </ac> . </para>\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(elmo_preprocess[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bio': ['O O O B I I I I I I I I I I I I I I I O B I I I I I I I I I I I I I I I I I I I O B I I I I I O',\n",
       "   'B I I I I I I I I I I I I I O B I I I I I I I I I I I I I O B I I I I I I I I I O O B I I I I I I I I I O O O B I I I I I I I I I I I I I I I I I I I I O O O O B I I I I I I I I I I I I O',\n",
       "   'B I I I I I I I I I I I I I I I I I I I I I I O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I O',\n",
       "   'B I I I I I I I I I I O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I O O B I I I I I I I I I I O O I I I I I I I I I I I I I I I I I I I I I I I I I I I O',\n",
       "   'B I I I I I I I I I I I I I I I O O B I I I I I I I I I I I I I I I I I I I O',\n",
       "   'O O O O O B I I I I I I I O O O O O O O B I I I I I I I I I I I O O O O',\n",
       "   'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B I O O O B I I I I I I I I I I I I O O O O O O B I I I I I I I I I I I I I I I O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I O O O O O O O O B I I I I I I I I B I I I I I I I I I I I I I I I I I I I I I I I I I I I I O',\n",
       "   'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O'],\n",
       "  'context': ['so clearly , we are living in a time where we feel justified to hate on \" illegals \" . the solution that the president-elect has been talking about for years is a giant wall on the us southern border . i see this solution as such :',\n",
       "   \"i have a giant block of cheese sitting in the middle of my house . mice will not stop getting into my house and eating this block of cheese ( i do't think mice are really into cheese like this , but i can't think of a better analogy at the moment ) . so my idea is to leave the block of cheese and try catching the mice and releasing them back outside my house . of course , they know the cheese is there and they keep coming for the cheese .\",\n",
       "   'the problem with undocumented workers coming into the country to take jobs is that there is someone offering these jobs to undocumented workers . construction crews , agriculture , etc. , are all industries where the leaders can save a whole lot of money by having undocumented workers , pay them salaries below minimum wage , offer no benefits , and threaten to have them deported .',\n",
       "   \"what i think we should do is go after these people . if instead of a giant wall and an insane increase in a task force , it would be a lot cheaper to have a regulative authority that regularly audits and investigates companies that engage in this practice . and companies that are found engaging in it should face severe punishment ... .i 'm talking massive fines for first offense and revoking of business licenses in other offenses , potentially increasing to the point of confiscating the business and property .\",\n",
       "   'i assume that there is already a bit of a penalty in place for these employers , but the risk of getting caught must still be cheaper than the cost of only hiring american workers at decent wages .',\n",
       "   \"edit : okay everyone , i object to using the word illegal to describe an entire group of people . it 's insensitive , offensive , and in the context , racist . ( link )\",\n",
       "   \"edit 2 : let me clarify my personal views . i think this country needs more immigration , not less . i think we need open borders and that we 're moving towards globalization and overall , we need to move away from the idea of american superiority . i truly do't believe that illegal immigration is a problem severe enough to warrant building a massive wall , though i do concede that it may be easier to try to keep people out than to try to regulate business . my main issue is the republican talking point , and one of the cornerstones of the president-elect 's campaign that blames undocumented workers for taking american jobs and making the country less safe . and i do believe that even though it 's not an easy thing to do , we should be working more to fight the people that wish to exploit desperate people than the people willing to do anything to improve the lives of their family .\",\n",
       "   \"> hello , users of cmv ! this is a footnote from your moderators . we 'd just like to remind you of a couple of things . firstly , please remember to read through our rules ( link ) . if you see a comment that has broken one , it is more effective to report it than downvote it . speaking of which , downvotes do't change views ( link ) ! if you are thinking about submitting a cmv yourself , please have a look through our popular topics wiki ( link ) first . any questions or concerns ? feel free to message us ( link ) . happy cmving !\"],\n",
       "  'type': ['C',\n",
       "   'C',\n",
       "   'C',\n",
       "   'P',\n",
       "   'P',\n",
       "   'P',\n",
       "   'P',\n",
       "   'C',\n",
       "   'P',\n",
       "   'C',\n",
       "   'P',\n",
       "   'C',\n",
       "   'P',\n",
       "   'P',\n",
       "   'C',\n",
       "   'C',\n",
       "   'C',\n",
       "   'C',\n",
       "   'C',\n",
       "   'C',\n",
       "   'C',\n",
       "   'C',\n",
       "   'C',\n",
       "   'C']},\n",
       " {'bio': ['B I I B I I I I I O B I I I I I I I I I I I I I I I I I I I I I I I I I I I O B I I I I I I I I I I I I I I I I I I I I I I I I O'],\n",
       "  'context': ['the problem is many illegals do\\'t work regular jobs . they work in fields and are paid cash , or they \\'re nannys , or the guy that came around asking to rake our leaves for $ 20 . many of them work \" gig \" type jobs where it \\'s unreasonable to expect people to have the resources to verify their immigration status .'],\n",
       "  'type': ['C', 'C', 'P', 'P']},\n",
       " {'bio': ['B I I I I I I I I I I O'],\n",
       "  'context': ['i disqualify this due to using the word \" illegals \" .'],\n",
       "  'type': ['C']},\n",
       " {'bio': ['B I I I I I I I I I B I I I I I I I I I I I I I I I I I I I'],\n",
       "  'context': ['that \\'s a fickle reason to dismiss a point . if you find/replace \" illegals \" with \" people in the country illegally \" do you find it convincing ?'],\n",
       "  'type': ['C', 'P']},\n",
       " {'bio': ['O O O O O O O B I I I I I I I I I O'],\n",
       "  'context': [\"based on op 's response here , obviously op is not looking to get his/her mind changed .\"],\n",
       "  'type': ['C']},\n",
       " {'bio': ['O O O O O O O O O O O O O O O O',\n",
       "   'B I I I I I I I I I I I I I I I B B I I I I I I I I I I I I I I I B I I O O O O O O O'],\n",
       "  'context': ['interesting you say that since further in the thread , i already awarded a delta .',\n",
       "   'the term \" illegals \" being used to define a group of people is offensive and since it is used ( definitely in this context ) to describe a particular racial group , it \\'s racist . ( link ) language matters .'],\n",
       "  'type': ['C', 'P', 'P', 'P']},\n",
       " {'bio': ['O O O O O O O O O O O O O O O O O O O O O O'],\n",
       "  'context': ['op maybe you can add in the description how you feel about the word , to avoid more instances popping up ?'],\n",
       "  'type': []},\n",
       " {'bio': ['O O O O O O O O'],\n",
       "  'context': ['and i just did . : - )'],\n",
       "  'type': []}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[side][topic_id]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9282\n",
      "[CLS] δ - this reasoning was very convincing , it always occurred to me that penetrating the box was a default offensive strategy and that therefore suspend ##ing offs ##ides in the box would not considerably change the tactics used on ##field . this perspective has helped change my mind . [SEP]\n",
      "\n",
      "1 2 [CLS]\n",
      "0 2 δ\n",
      "0 2 -\n",
      "0 2 this\n",
      "0 2 reasoning\n",
      "0 2 was\n",
      "0 2 very\n",
      "0 2 convincing\n",
      "0 2 ,\n",
      "0 2 it\n",
      "0 2 always\n",
      "0 2 occurred\n",
      "0 2 to\n",
      "0 2 me\n",
      "0 2 that\n",
      "0 0 penetrating\n",
      "0 1 the\n",
      "0 1 box\n",
      "0 1 was\n",
      "0 1 a\n",
      "0 1 default\n",
      "0 1 offensive\n",
      "0 1 strategy\n",
      "0 2 and\n",
      "0 2 that\n",
      "0 2 therefore\n",
      "0 2 suspend\n",
      "1 0 ##ing\n",
      "0 1 offs\n",
      "1 1 ##ides\n",
      "0 1 in\n",
      "0 1 the\n",
      "0 1 box\n",
      "0 1 would\n",
      "0 1 not\n",
      "0 1 considerably\n",
      "0 1 change\n",
      "0 1 the\n",
      "0 1 tactics\n",
      "0 1 used\n",
      "0 1 on\n",
      "1 2 ##field\n",
      "0 2 .\n",
      "0 2 this\n",
      "0 2 perspective\n",
      "0 2 has\n",
      "0 2 helped\n",
      "0 2 change\n",
      "0 2 my\n",
      "0 2 mind\n",
      "0 2 .\n",
      "2 2 [SEP]\n"
     ]
    }
   ],
   "source": [
    "side, index = 0, 9282\n",
    "\n",
    "c = index_mapping[side][index]\n",
    "for index, _ in enumerate(index_mapping[side]):\n",
    "    if(_ ==c):\n",
    "        print(index)\n",
    "        break\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(pred[side]['id'][index].tolist()[:len(pred[side]['bio'][index])])))\n",
    "print()\n",
    "for word, bio, r in zip(tokenizer.convert_ids_to_tokens(pred[side]['id'][index].tolist()), pred[side]['bio'][index], pred[side]['recover'][index].tolist()):\n",
    "    print(r,bio,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor bio, word in zip(posts[side][topic_id]['content'][0]['bio'][3].split(), posts[side][topic_id]['content'][0]['context'][3].split()):\\n    print(bio, word)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for bio, word in zip(posts[side][topic_id]['content'][0]['bio'][3].split(), posts[side][topic_id]['content'][0]['context'][3].split()):\n",
    "    print(bio, word)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9282\n",
      "0 10078\n",
      "0 10233\n",
      "0 20693\n",
      "0 20695\n",
      "0 21906\n",
      "0 24220\n",
      "0 24464\n",
      "0 25223\n",
      "0 31012\n",
      "0 31447\n",
      "0 32848\n",
      "0 33023\n",
      "0 33333\n",
      "0 33350\n",
      "0 37964\n",
      "0 38032\n",
      "0 38320\n",
      "0 39184\n",
      "0 39603\n",
      "0 39794\n",
      "0 44531\n",
      "0 45445\n",
      "0 47456\n",
      "0 47668\n",
      "0 48072\n",
      "0 49641\n",
      "0 49653\n",
      "0 50368\n",
      "0 50777\n",
      "0 51517\n",
      "0 54778\n",
      "0 57569\n",
      "0 58099\n",
      "0 60946\n",
      "0 61893\n",
      "0 62351\n",
      "0 64087\n",
      "0 64319\n",
      "0 65831\n",
      "0 65974\n",
      "0 69721\n",
      "0 71947\n",
      "0 72166\n",
      "0 73694\n",
      "0 73746\n",
      "0 74994\n",
      "0 74995\n",
      "0 75420\n",
      "0 77238\n",
      "0 78650\n",
      "0 78751\n",
      "0 79430\n",
      "0 84061\n",
      "0 84892\n",
      "0 86953\n",
      "0 87409\n",
      "0 87750\n",
      "0 88068\n",
      "0 88385\n",
      "0 88469\n",
      "0 88707\n",
      "0 88875\n",
      "0 96909\n",
      "0 100934\n",
      "0 103899\n",
      "0 104280\n",
      "0 104461\n",
      "0 106998\n",
      "0 107009\n",
      "0 107595\n",
      "0 109584\n",
      "0 111787\n",
      "0 112419\n",
      "0 113904\n",
      "0 119493\n",
      "0 119889\n",
      "0 120152\n",
      "0 120409\n",
      "0 120839\n",
      "0 121063\n",
      "0 121323\n",
      "0 121546\n",
      "0 125741\n",
      "0 125884\n",
      "0 126286\n",
      "0 126509\n",
      "0 127708\n",
      "0 128001\n",
      "0 128275\n",
      "0 128637\n",
      "0 128926\n",
      "0 129144\n",
      "0 131392\n",
      "0 133750\n",
      "0 134100\n",
      "0 134134\n",
      "0 134361\n",
      "0 134385\n",
      "0 134791\n",
      "0 139000\n",
      "0 139196\n",
      "0 146266\n",
      "0 147043\n",
      "0 154240\n",
      "0 156811\n",
      "0 157659\n",
      "0 158246\n",
      "0 158745\n",
      "0 159943\n",
      "0 160357\n",
      "0 160840\n",
      "0 161186\n",
      "0 161753\n",
      "0 161869\n",
      "0 161967\n",
      "0 163615\n",
      "0 167686\n",
      "0 167761\n",
      "0 172121\n",
      "0 172717\n",
      "0 174731\n",
      "0 178772\n",
      "0 179444\n",
      "0 180579\n",
      "0 182914\n",
      "0 183005\n",
      "0 183424\n",
      "0 190927\n",
      "0 191690\n",
      "0 192829\n",
      "0 195052\n",
      "0 203672\n",
      "0 210977\n",
      "0 211396\n",
      "0 213013\n",
      "0 213298\n",
      "0 213714\n",
      "0 214011\n",
      "0 214149\n",
      "0 214296\n",
      "0 214682\n",
      "0 214883\n",
      "0 215229\n",
      "0 217401\n",
      "0 221269\n",
      "0 221458\n",
      "0 225296\n",
      "0 225558\n",
      "0 225756\n",
      "0 226055\n",
      "0 226258\n",
      "0 226564\n",
      "0 226653\n",
      "0 227682\n",
      "0 232811\n",
      "0 233332\n",
      "0 235106\n",
      "0 235434\n",
      "0 235865\n",
      "0 238286\n",
      "0 241577\n",
      "0 242548\n",
      "0 243277\n",
      "0 243290\n",
      "0 248686\n",
      "0 258914\n",
      "0 262445\n",
      "0 264037\n",
      "0 267355\n",
      "0 271977\n",
      "0 272942\n",
      "0 273368\n",
      "0 275767\n",
      "0 277488\n",
      "0 277760\n",
      "0 277784\n",
      "0 280581\n",
      "0 283411\n",
      "0 289386\n",
      "0 292174\n",
      "0 292709\n",
      "0 296035\n",
      "0 296088\n",
      "0 296241\n",
      "0 296370\n",
      "0 296399\n",
      "0 296529\n",
      "0 296574\n",
      "0 296766\n",
      "0 296792\n",
      "0 297108\n",
      "0 300804\n",
      "0 302486\n",
      "0 302606\n",
      "0 302979\n",
      "0 304039\n",
      "0 308926\n",
      "0 310393\n",
      "0 312439\n",
      "0 318464\n",
      "1 88\n",
      "1 2965\n",
      "1 4385\n",
      "1 8559\n",
      "1 9089\n",
      "1 11083\n",
      "1 11221\n",
      "1 12534\n",
      "1 18243\n",
      "1 22997\n",
      "1 24414\n",
      "1 25521\n",
      "1 25885\n",
      "1 26035\n",
      "1 26681\n",
      "1 33705\n",
      "1 34048\n",
      "1 35576\n",
      "1 40551\n",
      "1 41054\n",
      "1 41808\n",
      "1 42325\n",
      "1 47554\n",
      "1 50582\n",
      "1 50632\n",
      "1 50924\n",
      "1 51267\n",
      "1 51334\n",
      "1 52389\n",
      "1 53090\n",
      "1 53910\n",
      "1 53938\n",
      "1 54495\n",
      "1 54960\n",
      "1 60748\n",
      "1 62111\n",
      "1 62690\n",
      "1 65092\n",
      "1 65821\n",
      "1 69344\n",
      "1 71232\n",
      "1 71768\n",
      "1 72308\n",
      "1 75878\n",
      "1 76167\n",
      "1 77118\n",
      "1 77265\n",
      "1 78648\n",
      "1 79170\n",
      "1 80159\n",
      "1 80160\n",
      "1 80230\n",
      "1 80700\n",
      "1 84160\n",
      "1 86895\n",
      "1 87653\n",
      "1 89613\n",
      "1 90924\n",
      "1 92536\n",
      "1 93012\n",
      "1 93458\n",
      "1 93884\n",
      "1 94167\n",
      "1 94272\n",
      "1 94603\n",
      "1 94659\n",
      "1 95085\n",
      "1 96536\n",
      "1 100591\n",
      "1 100635\n",
      "1 101324\n",
      "1 103588\n",
      "1 103944\n",
      "1 104586\n",
      "1 104759\n",
      "1 105652\n",
      "1 111007\n",
      "1 111469\n",
      "1 112006\n",
      "1 112858\n",
      "1 117397\n",
      "1 117426\n",
      "1 119167\n",
      "1 119866\n",
      "1 127569\n",
      "1 127950\n",
      "1 128356\n",
      "1 128824\n",
      "1 129037\n",
      "1 129229\n",
      "1 129474\n",
      "1 129756\n",
      "1 129809\n",
      "1 134703\n",
      "1 134848\n",
      "1 135192\n",
      "1 136145\n",
      "1 136619\n",
      "1 136737\n",
      "1 137073\n",
      "1 137388\n",
      "1 137674\n",
      "1 139337\n",
      "1 139600\n",
      "1 145254\n",
      "1 145740\n",
      "1 145981\n",
      "1 146165\n",
      "1 148295\n",
      "1 148671\n",
      "1 150013\n",
      "1 152567\n",
      "1 154346\n",
      "1 156631\n",
      "1 157128\n",
      "1 158059\n",
      "1 159097\n",
      "1 169080\n",
      "1 169506\n",
      "1 170968\n",
      "1 171526\n",
      "1 172219\n",
      "1 172806\n",
      "1 173493\n",
      "1 176353\n",
      "1 179104\n",
      "1 179214\n",
      "1 180363\n",
      "1 180682\n",
      "1 183268\n",
      "1 184310\n",
      "1 187265\n",
      "1 187504\n",
      "1 190791\n",
      "1 192030\n",
      "1 194956\n",
      "1 194958\n",
      "1 196084\n",
      "1 199157\n",
      "1 201849\n",
      "1 204633\n",
      "1 205226\n",
      "1 205316\n",
      "1 205805\n",
      "1 206061\n",
      "1 206733\n",
      "1 211412\n",
      "1 211778\n",
      "1 217904\n",
      "1 219630\n",
      "1 219755\n",
      "1 221754\n",
      "1 226122\n",
      "1 228527\n",
      "1 228790\n",
      "1 229172\n",
      "1 229426\n",
      "1 229620\n",
      "1 229627\n",
      "1 229748\n",
      "1 230129\n",
      "1 230360\n",
      "1 230808\n",
      "1 230865\n",
      "1 233293\n",
      "1 237147\n",
      "1 237474\n",
      "1 242253\n",
      "1 242662\n",
      "1 242900\n",
      "1 243092\n",
      "1 243778\n",
      "1 244282\n",
      "1 244775\n",
      "1 252703\n",
      "1 253075\n",
      "1 259389\n",
      "1 259644\n",
      "1 259772\n",
      "1 260702\n",
      "1 266800\n",
      "1 267151\n",
      "1 267295\n",
      "1 267568\n",
      "1 269360\n",
      "1 269360\n",
      "1 269480\n",
      "1 270214\n",
      "1 274032\n",
      "1 278643\n",
      "1 279007\n",
      "1 280321\n",
      "1 280648\n",
      "1 280847\n",
      "1 282745\n",
      "1 282867\n",
      "1 282919\n",
      "1 284450\n",
      "1 285287\n",
      "1 289084\n",
      "1 289091\n",
      "1 289847\n",
      "1 290629\n",
      "1 294011\n",
      "1 296403\n",
      "1 296719\n",
      "1 298645\n",
      "1 298739\n",
      "1 298888\n",
      "1 300498\n",
      "1 301880\n",
      "1 302262\n",
      "1 304148\n",
      "1 304473\n",
      "1 305250\n",
      "1 309676\n",
      "1 309780\n",
      "1 309813\n",
      "1 311279\n",
      "1 313227\n",
      "1 314996\n",
      "1 315421\n",
      "1 315483\n",
      "1 317127\n",
      "1 317139\n",
      "1 317807\n",
      "1 319734\n",
      "1 319866\n",
      "1 319919\n",
      "1 320088\n",
      "1 320107\n",
      "1 320246\n",
      "1 320393\n",
      "1 320512\n",
      "1 320775\n",
      "1 329793\n",
      "1 336259\n",
      "1 336645\n",
      "1 344878\n"
     ]
    }
   ],
   "source": [
    "for side in range(2):\n",
    "    for index in range(len(pred[side]['id'])):\n",
    "        last = [[2,1]]\n",
    "        acc = [[0,0,0]]\n",
    "        for word, bio, r in zip(tokenizer.convert_ids_to_tokens(pred[side]['id'][index].tolist()[1:]), pred[side]['bio'][index][1:], pred[side]['recover'][index].tolist()[1:]):\n",
    "            if(bio==0 and r>0 and word[:2]=='##'):\n",
    "                for _ in last:\n",
    "                    pass\n",
    "                    if(_[0] == 0 or _[1] =='.'):\n",
    "                        break\n",
    "                else:\n",
    "                    print(side, index)\n",
    "            #if(r>0):\n",
    "            #    last.append([bio, word])\n",
    "            if(r==0):\n",
    "                last = [[bio, word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so clearly , we are living in a time where we feel justified to hate on \" illegals \" . the solution that the president-elect has been talking about for years is a giant wall on the us southern border . i see this solution as such :', \"i have a giant block of cheese sitting in the middle of my house . mice will not stop getting into my house and eating this block of cheese ( i do't think mice are really into cheese like this , but i can't think of a better analogy at the moment ) . so my idea is to leave the block of cheese and try catching the mice and releasing them back outside my house . of course , they know the cheese is there and they keep coming for the cheese .\", 'the problem with undocumented workers coming into the country to take jobs is that there is someone offering these jobs to undocumented workers . construction crews , agriculture , etc. , are all industries where the leaders can save a whole lot of money by having undocumented workers , pay them salaries below minimum wage , offer no benefits , and threaten to have them deported .', \"what i think we should do is go after these people . if instead of a giant wall and an insane increase in a task force , it would be a lot cheaper to have a regulative authority that regularly audits and investigates companies that engage in this practice . and companies that are found engaging in it should face severe punishment ... .i 'm talking massive fines for first offense and revoking of business licenses in other offenses , potentially increasing to the point of confiscating the business and property .\", 'i assume that there is already a bit of a penalty in place for these employers , but the risk of getting caught must still be cheaper than the cost of only hiring american workers at decent wages .', \"edit : okay everyone , i object to using the word illegal to describe an entire group of people . it 's insensitive , offensive , and in the context , racist . ( link )\", \"edit 2 : let me clarify my personal views . i think this country needs more immigration , not less . i think we need open borders and that we 're moving towards globalization and overall , we need to move away from the idea of american superiority . i truly do't believe that illegal immigration is a problem severe enough to warrant building a massive wall , though i do concede that it may be easier to try to keep people out than to try to regulate business . my main issue is the republican talking point , and one of the cornerstones of the president-elect 's campaign that blames undocumented workers for taking american jobs and making the country less safe . and i do believe that even though it 's not an easy thing to do , we should be working more to fight the people that wish to exploit desperate people than the people willing to do anything to improve the lives of their family .\", \"> hello , users of cmv ! this is a footnote from your moderators . we 'd just like to remind you of a couple of things . firstly , please remember to read through our rules ( link ) . if you see a comment that has broken one , it is more effective to report it than downvote it . speaking of which , downvotes do't change views ( link ) ! if you are thinking about submitting a cmv yourself , please have a look through our popular topics wiki ( link ) first . any questions or concerns ? feel free to message us ( link ) . happy cmving !\"]\n"
     ]
    }
   ],
   "source": [
    "for _ in clean_posts[0]:\n",
    "    if( _['uid']=='4_0'):\n",
    "        print(_['context'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"you are giving way too much power to the american president . it 's just a person with little power . the system behind him/her is much stronger and much more powerful . any drastic measure that goes against the stability and harmony of the system ( like using nuclear weapons ) would be stopped before it happened .\",\n",
       " \"the us will absolutely not go into civil war because its economy is performing well . if there 's jobs and economical prosperity , there 's no need for war . trump supporters are loud , but they are weak willed and minded . if he looses they 'll just go back home .\"]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_posts[0][3589]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 what\n",
      "0 1 i\n",
      "0 1 think\n",
      "0 1 we\n",
      "0 1 should\n",
      "0 1 do\n",
      "0 1 is\n",
      "0 1 go\n",
      "0 1 after\n",
      "0 1 these\n",
      "0 1 people\n",
      "0 2 .\n",
      "0 0 if\n",
      "0 1 instead\n",
      "0 1 of\n",
      "0 1 a\n",
      "0 1 giant\n",
      "0 1 wall\n",
      "0 1 and\n",
      "0 1 an\n",
      "0 1 insane\n",
      "0 1 increase\n",
      "0 1 in\n",
      "0 1 a\n",
      "0 1 task\n",
      "0 1 force\n",
      "0 1 ,\n",
      "0 1 it\n",
      "0 1 would\n",
      "0 1 be\n",
      "0 1 a\n",
      "0 1 lot\n",
      "0 1 cheaper\n",
      "0 1 to\n",
      "0 1 have\n",
      "0 1 a\n",
      "0 1 reg\n",
      "1 1 ##ula\n",
      "2 1 ##tive\n",
      "0 1 authority\n",
      "0 1 that\n",
      "0 1 regularly\n",
      "0 1 audit\n",
      "1 1 ##s\n",
      "0 1 and\n",
      "0 1 investigates\n",
      "0 1 companies\n",
      "0 1 that\n",
      "0 1 engage\n",
      "0 1 in\n",
      "0 1 this\n",
      "0 1 practice\n",
      "0 2 .\n",
      "0 2 and\n",
      "0 0 companies\n",
      "0 1 that\n",
      "0 1 are\n",
      "0 1 found\n",
      "0 1 engaging\n",
      "0 1 in\n",
      "0 1 it\n",
      "0 1 should\n",
      "0 1 face\n",
      "0 1 severe\n",
      "0 1 punishment\n",
      "0 2 .\n",
      "1 2 .\n",
      "2 2 .\n",
      "0 2 .\n",
      "1 0 i\n",
      "0 1 '\n",
      "1 1 m\n",
      "0 1 talking\n",
      "0 1 massive\n",
      "0 1 fines\n",
      "0 1 for\n",
      "0 1 first\n",
      "0 1 offense\n",
      "0 1 and\n",
      "0 1 rev\n",
      "1 1 ##oki\n",
      "2 1 ##ng\n",
      "0 1 of\n",
      "0 1 business\n",
      "0 1 licenses\n",
      "0 1 in\n",
      "0 1 other\n",
      "0 1 offenses\n",
      "0 1 ,\n",
      "0 1 potentially\n",
      "0 1 increasing\n",
      "0 1 to\n",
      "0 1 the\n",
      "0 1 point\n",
      "0 1 of\n",
      "0 1 con\n",
      "1 1 ##fi\n",
      "2 1 ##sca\n",
      "3 1 ##ting\n",
      "0 1 the\n",
      "0 1 business\n",
      "0 1 and\n",
      "0 1 property\n",
      "0 2 .\n",
      "2 2 [SEP]\n"
     ]
    }
   ],
   "source": [
    "side, index = 0, 443\n",
    "for word, bio, r in zip(tokenizer.convert_ids_to_tokens(pred[side]['id'][index].tolist()[1:]), pred[side]['bio'][index][1:], pred[side]['recover'][index].tolist()[1:]):\n",
    "    print(r,bio,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
