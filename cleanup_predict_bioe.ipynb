{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code aims at converting tensor back to text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import collections\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "import string\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./saved_models/gaku_essay_55/', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/nfs/nas-5.1/kyhuang/preprocess/cmv_raw_origin_v4/heldout/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = {}\n",
    "index_mapping = {}\n",
    "\n",
    "for side, key in enumerate(['pos', 'neg', 'op']):\n",
    "    pred[key] = torch.load('./pred_result/origin_v4/heldout/gaku_essay_{}'.format(key))\n",
    "    with open('./mapping/origin_v4/heldout/mapping_clean_{}'.format(key)) as f:\n",
    "        index_mapping[key] = [line for line in f]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_type = copy.deepcopy(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkandvote(sent, recovers, bios, dtypes):\n",
    "    mapping_bio = ['B', 'I', 'O', 'E']\n",
    "    mapping_type = ['P', 'C']\n",
    "    \n",
    "    \n",
    "    recover_sent, recover_bio = [], []\n",
    "    adu_type = []\n",
    "    count = [0, 0]\n",
    "    \n",
    "    last = torch.rand(2)\n",
    "    for index, (word, r, bio, dtype) in enumerate(zip(sent[1:], recovers, bios, dtypes)):\n",
    "        if(word[:2]=='##'):\n",
    "            if(bio==0):\n",
    "                last = torch.zeros_like(dtype)\n",
    "                \n",
    "                recover_bio[-1] = 'B'\n",
    "                count[0] += 1\n",
    "                \n",
    "            if( bio == 3 ):\n",
    "                if( recover_bio[-1] == 'B' ):\n",
    "                    recover_bio[-1] = 'O'\n",
    "                    count[0] -= 1\n",
    "                else:\n",
    "                    adu_type.append(\n",
    "                        mapping_type[last.argmax(-1).item()]\n",
    "                    )\n",
    "                    recover_bio[-1] = mapping_bio[bio]\n",
    "                    count[1] += 1\n",
    "                \n",
    "            recover_sent[-1] += word.split('#')[-1]\n",
    "        else:\n",
    "            if( bio == 0 ):\n",
    "                last = torch.zeros_like(dtype)\n",
    "                count[0] += 1\n",
    "                \n",
    "            elif( bio == 3 ):\n",
    "                adu_type.append(\n",
    "                    mapping_type[last.argmax(-1).item()]\n",
    "                )\n",
    "                count[1] += 1\n",
    "\n",
    "            recover_sent.append(word)\n",
    "            recover_bio.append(mapping_bio[bio])\n",
    "\n",
    "        if(bio<=1):\n",
    "            last += dtype\n",
    "    \n",
    "    if(count[1] != count[0]):\n",
    "        adu_type.append(\n",
    "            mapping_type[last.argmax(-1).item()]\n",
    "        )\n",
    "        count[1] +=1\n",
    "        recover_bio[-1] = 'O'\n",
    "        if(recover_bio[-2] == 'B'):\n",
    "            recover_bio[-2] = 'O'\n",
    "        else:\n",
    "            recover_bio[-2] = 'E'\n",
    "        \n",
    "    assert count[0]==(len(adu_type)),\"{}\\n{}\\n{}\".format(bios, recover_bio, adu_type)\n",
    "    assert count[1]==(len(adu_type)),\"{}\\n{}\\n{}\".format(bios, recover_bio, adu_type)\n",
    "    return recover_sent, recover_bio, adu_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recover = {}\n",
    "for key in ['pos', 'neg', 'op']:\n",
    "    recover[key] = {'sent':[], 'bio':[], 'index':[],'type':[]}\n",
    "    for index in range(len(pred[key]['id'])):\n",
    "        temp = checkandvote(\n",
    "                        tokenizer.convert_ids_to_tokens(pred[key]['id'][index].tolist()), \n",
    "                        pred[key]['recover'][index].tolist(), pred[key]['bio'][index],\n",
    "                        torch.tensor(pred_type[key]['type'][index]).softmax(-1)\n",
    "                       )\n",
    "        \n",
    "        recover[key]['sent'].append(' '.join(temp[0]))\n",
    "        recover[key]['bio'].append(temp[1])\n",
    "        recover[key]['type'].append(temp[2])\n",
    "        recover[key]['index'].append(pred[key]['index'][index].item())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_recover = copy.deepcopy(recover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def printorigin(side, index):\n",
    "    for word, r, bio in zip(tokenizer.convert_ids_to_tokens(pred[side]['id'][index].tolist()[1:]), pred[side]['recover'][index][:-2], pred[side]['bio'][index][:-2]):\n",
    "        print(bio, r, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def printrecover(recover, side, index):\n",
    "    print(recover[side]['type'][index])\n",
    "    for word, bio in zip(recover[side]['sent'][index].split(),recover[side]['bio'][index]):\n",
    "        print(word, bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find next token is that\n",
    "def checkandcollect(recover, key):\n",
    "    rm_stat = collections.defaultdict(int)\n",
    "\n",
    "    for side in recover:\n",
    "        for r_index in range(len(recover[side]['bio'])):\n",
    "            buffer = []\n",
    "            sent = recover[side]['sent'][r_index].split()\n",
    "            bios = recover[side]['bio'][r_index]\n",
    "            \n",
    "            for index, (word, bio) in enumerate( zip(sent[:-1], bios) ):\n",
    "                if( bio =='I' and word =='because'):\n",
    "                    print(sent)\n",
    "                    print(bios)\n",
    "                    None +1\n",
    "                    \n",
    "    return rm_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkkey(recover, key):\n",
    "    for side in recover:\n",
    "        for r_index in range(len(recover[side]['bio'])):\n",
    "            buffer = []\n",
    "            sent = recover[side]['sent'][r_index].split()\n",
    "            bios = recover[side]['bio'][r_index]\n",
    "\n",
    "            for index, (word, bio) in enumerate( zip(sent[:-1], bios) ):\n",
    "                if( bio =='I' and word ==key):\n",
    "                    print(' '.join(sent))\n",
    "                    print(sent[max(-24+index,0):index+8])\n",
    "                    print(bios[max(-24+index,0):index+8])\n",
    "                    None +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>', 'thanks', 'for', 'the', 'kind', 'words', '.', 'i', 'appreciate', 'it', '.', 'i', 'also', 'share', 'your', 'concern', 'for', 'our', 'country', ',', 'and', 'agree', 'wholeheartedly', 'that', 'every', 'vote', 'counts', '.', 'however', ',', 'i', \"'\", 'm', 'afraid', 'i', 'can', \"'\", 't', 'encourage', 'millions', 'of', 'people', 'whom', 'i', \"'\", 've', 'never', 'met', 'to', 'just', 'run', 'out', 'and', 'cast', 'a', 'ballot', ',', 'simply', 'because', 'they', 'have', 'the', 'right', 'to', 'vote', '.', 'that', 'would', 'be', 'like', 'encouraging', 'everyone', 'to', 'buy', 'an', 'ar', '-', '15', ',', 'simply', 'because', 'they', 'have', 'the', 'right', 'to', 'bear', 'arms', '.', 'i', 'would', 'need', 'to', 'know', 'a', 'few', 'things', 'about', 'them', 'before', 'offering', 'that', 'kind', 'of', 'encouragement', '.', 'for', 'instance', ',', 'do', 'they', 'know', 'how', 'to', 'care', 'for', 'a', 'weapon', '?', 'can', 'they', 'afford', 'the', 'cost', 'of', 'the', 'weapon', '?', 'do', 'they', 'have', 'a', 'history', 'of', 'violence', '?', 'are', 'they', 'mentally', 'stable', '?', 'in', 'short', ',', 'are', 'they', 'responsible', 'citizens', '?']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'B', 'I', 'I', 'I', 'E', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'E']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c1e7c05789fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrm_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrm_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'that'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckandcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecover\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'that'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrm_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'whether'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckandcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecover\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'whether'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrm_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'if'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckandcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecover\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'if'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrm_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckandcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecover\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-eae75e948d02>\u001b[0m in \u001b[0;36mcheckandcollect\u001b[0;34m(recover, key)\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbios\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0;32mNone\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrm_stat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "rm_sent = {}\n",
    "rm_sent['that'] = checkandcollect(recover, 'that')\n",
    "rm_sent['whether'] = checkandcollect(recover, 'whether')\n",
    "rm_sent['if'] = checkandcollect(recover, 'if')\n",
    "rm_sent['is'] = checkandcollect(recover, 'is')\n",
    "rm_sent['are'] = checkandcollect(recover, 'are')\n",
    "rm_sent['which'] = checkandcollect(recover, 'which')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def checkandprint(check, arr, windex, sent, bios):\n",
    "    key = 'because of'\n",
    "    l = len(key.split())\n",
    "    c = ' '.join(arr[:l])\n",
    "    \n",
    "    if(c == key):\n",
    "        print()\n",
    "        print(arr)\n",
    "        print(bios[windex:windex+len(arr)])\n",
    "        \n",
    "        for i in range(1, min(len(arr), 6)):\n",
    "            check[' '.join(arr[:i])] += 1\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "check = collections.defaultdict(int)\n",
    "length = 8\n",
    "c = 0\n",
    "for side in range(2):\n",
    "    for index in range(len(recover[side]['sent'])):\n",
    "        buffer = []\n",
    "        sent = temp_recover[side]['sent'][index].split()\n",
    "        bios =  temp_recover[side]['bio'][index]\n",
    "        for windex, (word, bio) in enumerate(zip(sent, bios)):\n",
    "            if(bio != 'O'):\n",
    "                buffer.append(word)\n",
    "                \n",
    "                if( bio == 'E' ):\n",
    "                    if(checkandprint(check, buffer, windex-len(buffer)+1, sent, bios)):\n",
    "                        print(side, index, windex)\n",
    "                        c += 1\n",
    "                    buffer = []\n",
    "c                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted([(key, val) for key, val in check.items()], key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length = 8\n",
    "stat = []\n",
    "for _ in range(2):\n",
    "    stat.append([collections.defaultdict(int) for _ in range(length)])\n",
    "    \n",
    "def adddict(dataset, words, length, front=True):\n",
    "    l = min(len(buffer), length)\n",
    "    if(front):\n",
    "        dataset[l-1][' '.join(buffer[:l])] += 1\n",
    "    else:\n",
    "        dataset[l-1][' '.join(buffer[-l:])] += 1\n",
    "    \n",
    "for side in range(2):\n",
    "    for index in range(len(recover[side]['sent'])):\n",
    "        buffer = []\n",
    "        for word, bio in zip(recover[side]['sent'][index].split(),recover[side]['bio'][index]):\n",
    "            if(bio == 'B' or bio == 'I'):\n",
    "                buffer.append(word)\n",
    "                                        \n",
    "            if(bio == 'E'):\n",
    "                buffer.append(word)\n",
    "                #if(len(buffer) == 2):\n",
    "                #    print(side, index)\n",
    "                adddict(stat[0], buffer, length, True)\n",
    "                adddict(stat[1], buffer, length, False)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(length):\n",
    "        stat[i][j] = sorted([(key, value) for key, value in stat[i][j].items()], key = lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 this is problematic\n",
      "1 a major problem\n",
      "1 lots of problems\n",
      "1 thats the problem\n",
      "1 there are problems\n",
      "1 both are problems\n",
      "1 it becomes problematic\n",
      "1 they are problematic\n",
      "23 this is the problem\n",
      "19 this is a problem\n",
      "6 that is a problem\n",
      "4 problems with mars 1\n",
      "4 this is my problem\n",
      "4 these problems are necessary\n",
      "4 it is a problem\n",
      "4 my problem with it\n",
      "3 i have a problem\n",
      "3 overpopulation is a problem\n",
      "2 i have this problem\n",
      "2 obesity causes health problems\n",
      "2 this is your problem\n",
      "2 that is their problem\n",
      "2 one of the problems\n",
      "2 there are several problems\n",
      "2 the problem is regional\n",
      "2 the problem was ricky\n",
      "2 there are other problems\n",
      "2 inflation is a problem\n",
      "2 the problem is support\n",
      "2 radicalization is a problem\n",
      "2 speceisism is inherently problematic\n",
      "2 the problems were addressed\n",
      "1 the problem is risk\n",
      "1 i have memory problems\n",
      "1 the problem with violence\n",
      "1 the problem is huge\n",
      "1 another problem is space\n",
      "1 trump represents the problem\n",
      "1 it is incredibly problematic\n",
      "1 which is my problem\n",
      "1 not even a problem\n",
      "1 the internet connection problem\n",
      "1 speed is a problem\n",
      "1 real - life problems\n",
      "1 the handshake is problematic\n",
      "1 i see a problem\n",
      "1 serious problem in bolivia\n",
      "1 i find it problematic\n",
      "1 it amplifies other problems\n",
      "1 the problem is deceit\n",
      "1 there some major problems\n",
      "1 the problem is culture\n",
      "1 your problem is illinois\n",
      "1 those have some problems\n",
      "1 that is the problem\n",
      "1 i see the problem\n",
      "1 adjustment would be problematic\n",
      "1 people are having problems\n",
      "1 it causes a problem\n",
      "1 the problem with clonability\n",
      "1 those problems are relevant\n",
      "1 monopsony is the problem\n",
      "1 closed source software problem\n",
      "1 word problems are hard\n",
      "1 there is a problem\n",
      "1 concrete problems with sports\n",
      "1 just an engineering problem\n",
      "1 they are trivial problems\n",
      "1 the problem with korra\n",
      "35 issues of consent are problematic\n",
      "31 the biggest problem is overpopulation\n",
      "10 that ' s the problem\n",
      "5 the problem with your argument\n",
      "4 irresponsibility and causing psychological problems\n",
      "4 it ' s a problem\n",
      "4 this is problematic is obvious\n",
      "4 what problems with violence ?\n",
      "4 the problem with np links\n",
      "3 addiction is a real problem\n",
      "3 it does prevent one problem\n",
      "2 that is really the problem\n",
      "2 as for the overall problem\n",
      "2 congressional meddling is a problem\n",
      "2 islamic fundamentalism is a problem\n",
      "2 counterfeiting would be a problem\n",
      "2 there ' s your problem\n",
      "2 one of the biggest problems\n",
      "2 there ' s a problem\n",
      "2 current day america has problems\n",
      "2 this is a huge problem\n",
      "2 student debt is a problem\n",
      "2 this also creates a problem\n",
      "2 my biggest problem with her\n",
      "2 troubleshoot all major computer problems\n",
      "2 the inherent problem with hft\n",
      "2 meditation will solve these problems\n",
      "2 diversity and heterogeneity creates problems\n",
      "2 this would solve two problems\n",
      "2 how problematic is it really\n",
      "2 this is a big problem\n",
      "2 there are problems with trump\n",
      "2 energy is not the problem\n",
      "2 health problems . especially diabetes\n",
      "2 why is that a problem\n",
      "2 micromanaging markets is a problem\n",
      "2 it is a hard problem\n",
      "2 that is a problem too\n",
      "2 that ' s my problem\n",
      "2 the problem is in causation\n",
      "2 i still find it problematic\n",
      "2 solving the problem is easy\n",
      "1 containment is the other problem\n",
      "1 the problem with this theory\n",
      "1 this is your main problem\n",
      "1 lots of problems in here\n",
      "1 your alien analogy is problematic\n",
      "1 the problem with your proposal\n",
      "1 the capitalism had its problems\n",
      "1 where is the problem ?\n",
      "1 this would be a problem\n",
      "1 the problem with legalized prostitution\n",
      "1 it has problems beyond that\n",
      "1 why is this a problem\n",
      "1 brigading is indeed a problem\n",
      "1 these are not actual problems\n",
      "1 the actual problem is overcrowding\n",
      "1 they just delay the problem\n",
      "1 the problem with legalising prostitution\n",
      "1 all problems are not equal\n",
      "1 there are problems within society\n",
      "1 this wraps the problem nicely\n",
      "1 this is exactly the problem\n",
      "1 this will be a problem\n",
      "1 i see the problem now\n",
      "1 i ' ll tackle problematic\n",
      "1 that ' s a problem\n",
      "1 the investors problem is huge\n",
      "1 this is a real problem\n",
      "1 this is the biggest problem\n",
      "1 it may create more problems\n",
      "1 we should solve the problem\n",
      "1 the problem with fat taxes\n",
      "1 there are problems with trafficking\n",
      "1 this problem has remained static\n",
      "1 the problem is all semantics\n",
      "1 the problem with vertical videos\n",
      "1 the problem with this interpretation\n",
      "1 eminent domain and holdout problems\n",
      "1 automation will cause economic problems\n",
      "1 the problem is not islam\n",
      "1 money was not the problem\n",
      "1 the problem with assisted suicide\n",
      "1 removing votes is also problematic\n",
      "1 male circumcision is a problem\n",
      "1 is not necessarily a problem\n",
      "1 terrorism worldwide is a problem\n",
      "1 here ' s my problem\n",
      "1 the beard was a problem\n",
      "1 it ' s our problem\n",
      "1 that is a fundamental problem\n",
      "1 in terms of prioritizing problems\n",
      "1 there is a race problem\n",
      "1 this is certainly a problem\n",
      "1 this never solves the problem\n",
      "1 there is one mayor problem\n",
      "1 that is not the problem\n",
      "1 this is the main problem\n",
      "1 religious beliefs are a problem\n",
      "1 piracy has a moral problem\n",
      "1 india has a rape problem\n",
      "1 this is the essential problem\n",
      "1 the sentence is a problem\n",
      "1 the problem with the test\n",
      "1 i still have these problems\n",
      "1 this was my main problem\n",
      "1 the problem with conspiracy theories\n",
      "1 this is actually a problem\n",
      "1 the problem with a republic\n",
      "1 it is a serious problem\n",
      "1 those refugees have a problem\n",
      "1 europe has a migrant problem\n",
      "1 this is the whole problem\n",
      "1 that could be a problem\n",
      "1 the problem lies in interpretation\n",
      "1 this is a temporary problem\n",
      "1 both can be interpreted problematically\n",
      "1 second time around no problems\n",
      "1 my few problems with that\n",
      "1 there ' s the problem\n",
      "1 resource scarcity is another problem\n",
      "1 superman is not the problem\n",
      "1 one problem with your mindset\n",
      "1 barbie is not the problem\n",
      "1 the problem with this argument\n",
      "1 here ' s the problem\n",
      "1 cgi has a believability problem\n",
      "1 the problem with your analogy\n",
      "1 these problems begin at home\n",
      "1 it also creates a problem\n",
      "1 there are a few problems\n",
      "1 another alleged problem of presidentialism\n",
      "1 absolute meritocracy is a problem\n",
      "1 psychology can have similar problems\n",
      "1 gerrymandering is not a problem\n",
      "107 it brings different set of problems\n",
      "17 those are my favorite problematic amendments\n",
      "8 in greece there was a problem\n",
      "7 what ' s the problem ?\n",
      "7 this is happening and is problematic\n",
      "6 a good analogy for this problem\n",
      "6 public publishing is the main problem\n",
      "5 these problems are experiments in psychology\n",
      "5 the problem is deeper than policy\n",
      "5 i have no problem with that\n",
      "5 homeopathic doctors would be a problem\n",
      "4 each of these has its problems\n",
      "4 this is a problem for me\n",
      "4 something like opioids would be problematic\n",
      "4 why is this a problem ?\n",
      "4 the problem i have with it\n",
      "4 the problem is the currency union\n",
      "4 this seems like a minor problem\n",
      "4 it ' s a minor problem\n",
      "4 having a private investigator is problematic\n",
      "4 it ' s pointless and problematic\n",
      "4 human ignorance is a solvable problem\n",
      "4 the problem is not the username\n",
      "4 this is where the problem stems\n",
      "3 self - labeling can be problematic\n",
      "3 firearm related deaths are a problem\n",
      "3 the problem lies with the extremists\n",
      "3 that ' s the real problem\n",
      "3 i see where the problem is\n",
      "2 the problem comes with homogenized crops\n",
      "2 many complex problems have complex causes\n",
      "2 there are two problems with conformity\n",
      "2 syrian refugees are not our problem\n",
      "2 the problem with this is prejudgement\n",
      "2 maybe that would solve the problem\n",
      "2 it is a very annoying problem\n",
      "2 , creates an even bigger problem\n",
      "2 that leads to my second problem\n",
      "2 this is not a new problem\n",
      "2 is not that already a problem\n",
      "2 why is that my problem ?\n",
      "2 fear is a problem as well\n",
      "2 my problem is the three combined\n",
      "2 i consider climate change a problem\n",
      "2 there ' s a huge problem\n",
      "2 many states encounter the same problems\n",
      "2 the big bang is the problem\n",
      "2 the problem is not just autism\n",
      "2 this is where the problem lies\n",
      "2 it does not cause any problems\n",
      "2 the problem with this is ?\n",
      "2 these problems are huge by themselves\n",
      "2 thats part of the problem here\n",
      "2 hunger is a food distribution problem\n",
      "2 that is not a fundamental problem\n",
      "2 this is part of the problem\n",
      "2 why are animals now a problem\n",
      "2 that ' s not the problem\n",
      "2 this is the problem & mdash\n",
      "2 it ' s the same problem\n",
      "2 this right here is my problem\n",
      "2 why is that a problem ?\n",
      "2 boehner is not the problem here\n",
      "2 people are ' t the problem\n",
      "2 it ' s a hard problem\n",
      "2 i ' ve found the problem\n",
      "2 this is my problem with it\n",
      "2 acute health emergencies are another problem\n",
      "2 the problem is not with christmas\n",
      "2 this seems like a personal problem\n",
      "2 this falls under a different problem\n",
      "1 i see a few problems here\n",
      "1 lyme disease is a health problem\n",
      "1 > does not solve the problem\n",
      "1 this is a mental problem ?\n",
      "1 this could indeed be my problem\n",
      "1 similar to the baby boom problem\n",
      "1 this is not really a problem\n",
      "1 that is also problematic to me\n",
      "1 there are no problems teaching bilingually\n",
      "1 i have no problem with facetiousness\n",
      "1 there would be problems with stability\n",
      "1 also lots of problems in here\n",
      "1 terrorism is not a serious problem\n",
      "1 it ' s a problem here\n",
      "1 the problem is with the law\n",
      "1 here lies most of the problems\n",
      "1 there are probably other major problems\n",
      "1 the problem with incest is consent\n",
      "1 it does not fix the problem\n",
      "1 there is a few problems here\n",
      "1 abortion is a problem of dualism\n",
      "1 the first problem with your argument\n",
      "1 granting fan licenses is a problem\n",
      "1 this is only half the problem\n",
      "1 it ' s a tricky problem\n",
      "1 fantastical racism would be a problem\n",
      "1 this is exactly our main problem\n",
      "1 performance problems can be developed around\n",
      "1 that seems like a problematic approach\n",
      "1 which is part of the problem\n",
      "1 you point out a real problem\n",
      "1 i see four problems with this\n",
      "1 the problems are polarization and entrenchment\n",
      "1 it is a more nuanced problem\n",
      "1 there are some problems with democracy\n",
      "1 logistics is not the only problem\n",
      "1 i see how this is problematic\n",
      "1 this is not a small problem\n",
      "1 i understand the last mile problem\n",
      "1 quantum randomness really is a problem\n",
      "1 religious ideas are not the problem\n",
      "1 i have used steam without problems\n",
      "1 i dealt with a similar problem\n",
      "1 the problem is the supply side\n",
      "1 that does present a significant problem\n",
      "1 this is the same causal problem\n",
      "1 they presently have their own problems\n",
      "1 society has another problem - you\n",
      "1 some social problems would still exist\n",
      "1 i do have body image problems\n",
      "1 that can actually be a problem\n",
      "1 this problem is largely our fault\n",
      "1 the fourth point is truly problematic\n",
      "1 i was aware of this problem\n",
      "1 that ' s basically my problem\n",
      "1 practical relevance is the big problem\n",
      "1 one of the problems with racists\n",
      "1 you have never encountered this problem\n",
      "1 when does it become a problem\n",
      "1 i have a problem with this\n",
      "1 is not that the problem itself\n",
      "1 it ' s just too problematic\n",
      "1 political corruption is a serious problem\n",
      "1 it does not cause social problems\n",
      "1 this is a collective action problem\n",
      "1 there is also a confounding problem\n",
      "1 consider for instance the trolley problem\n",
      "1 my problem with all of this\n",
      "1 the geographical thing is a problem\n",
      "1 the length of time is problematic\n",
      "1 there is the problem of testing\n",
      "1 another problem with the taxation solution\n",
      "1 the problem is not with condorcet\n",
      "1 there is the problem right there\n",
      "1 is this actually a serious problem\n",
      "1 i have a problem forgetting things\n",
      "1 why is that such a problem\n",
      "1 there ' s a problem here\n",
      "1 the problem is not greedy corporations\n",
      "1 reasoned debate solves this problem better\n",
      "1 voter purging is a genuine problem\n",
      "1 a few problems with your premise\n",
      "1 it can be a problematic mentality\n",
      "1 the problem is that this compounds\n",
      "1 downvote brigades could become a problem\n",
      "1 you pile problems onto a therapist\n",
      "1 that ' s not a problem\n",
      "1 world hunger is a distribution problem\n",
      "1 there ' s another problem here\n",
      "1 that ' s exactly the problem\n",
      "1 i have a few problems here\n",
      "1 grade inflation might be a problem\n",
      "1 the problem is your interpretative model\n",
      "1 that ' s the basic problem\n",
      "1 debt seems to be a problem\n",
      "1 blame everyone else for your problems\n",
      "1 there are these problems with it\n",
      "1 i have no problem with preordering\n",
      "1 this is a valid logistical problem\n",
      "1 the genitalia argument has two problems\n",
      "1 there is similar problem with depression\n",
      "1 do you see the problem there\n",
      "1 this is part of my problem\n",
      "1 crazy long sentences are a problem\n",
      "1 misleading advertising is the larger problem\n",
      "1 this is the problem right here\n",
      "1 my problem with allowing steroid use\n",
      "1 not being immune is a problem\n",
      "1 guns do ' t fix problems\n",
      "1 technology itself might solve the problem\n",
      "1 how is this a problem ?\n",
      "1 how are these problems solved ?\n",
      "1 there is your problem , op\n",
      "1 this would not aid the problem\n",
      "1 no problem , happy to help\n",
      "1 i have no problem with abortion\n",
      "1 you might not have that problem\n",
      "1 what you are asking is problematic\n",
      "1 the problem , in my eyes\n"
     ]
    }
   ],
   "source": [
    "for _ in stat[0][:6]:\n",
    "    for key, val in _:\n",
    "        if('problem' in key):\n",
    "            print(val, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_word = [collections.defaultdict(int), collections.defaultdict(int)]\n",
    "\n",
    "for side in range(2):\n",
    "    for index in range(len(recover[side]['sent'])):\n",
    "        for word, bio in zip(recover[side]['sent'][index].split(),recover[side]['bio'][index]):\n",
    "            if(bio == 'B'):\n",
    "                stat_word[0][word] += 1\n",
    "            elif(bio == 'E'):\n",
    "                stat_word[1][word] += 1\n",
    "\n",
    "sort_stat_word = [None, None]\n",
    "for i in range(2):\n",
    "    sort_stat_word[i] = sorted([(key, val) for key, val in stat_word[i].items()],key= lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_fe_punctation(sent, bios, dtypes):\n",
    "    #print(sent)\n",
    "    #print(bios)\n",
    "    punc = '!#$%&\\*+,-./:;?@^_`|~'\n",
    "    \n",
    "    new_type = []\n",
    "    type_index = 0\n",
    "    \n",
    "    for index in range(len(sent)-1):\n",
    "        if(bios[index] == 'B'):\n",
    "            if(sent[index] in punc):\n",
    "                bios[index] = 'O'\n",
    "                if(bios[index+1] == 'E'):\n",
    "                    bios[index+1] = 'O'\n",
    "                    type_index += 1\n",
    "                else:\n",
    "                    bios[index+1] = 'B'\n",
    "        elif(bios[index] == 'E'):\n",
    "            new_type.append( dtypes[type_index])\n",
    "            type_index += 1\n",
    "    else:\n",
    "        if(bios[len(sent)-1] == 'E'):\n",
    "            new_type.append( dtypes[type_index])\n",
    "    # return sent, bios, new_type\n",
    "    \n",
    "    new_new_type = []\n",
    "    type_index = len(new_type)-1\n",
    "    for index in range(len(sent)-1, 0, -1):\n",
    "        if(bios[index] == 'E'):\n",
    "            if(sent[index] in punc):\n",
    "                bios[index] = 'O'\n",
    "                if(bios[index-1] == 'B'):\n",
    "                    bios[index-1] = 'O'\n",
    "                    type_index -= 1\n",
    "                else:\n",
    "                    bios[index-1] = 'E'\n",
    "        elif(bios[index] == 'B'):\n",
    "            new_new_type.append( new_type[type_index])\n",
    "            type_index -= 1\n",
    "    else:\n",
    "        if(bios[0] == 'B'):\n",
    "            new_new_type.append( new_type[type_index])\n",
    "                \n",
    "    return ' '.join(sent), bios, new_new_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_last_conj(sent, bios, dtypes):\n",
    "    #print(sent)\n",
    "    #print(bios)\n",
    "    #print(dtypes)\n",
    "    conj = ['and', 'or']\n",
    "    \n",
    "    new_type = []\n",
    "    type_index = len(dtypes)-1\n",
    "    \n",
    "    for index in range(len(sent)-1, 0, -1):\n",
    "        if(bios[index] == 'E'):\n",
    "            if(sent[index] in conj):\n",
    "                bios[index] = 'O'\n",
    "                if(bios[index-1] == 'B'):\n",
    "                    bios[index-1] = 'O'\n",
    "                    type_index -= 1\n",
    "                else:\n",
    "                    bios[index-1] = 'E'\n",
    "        elif(bios[index] == 'B'):\n",
    "            new_type.append( dtypes[type_index])\n",
    "            type_index -= 1\n",
    "    else:\n",
    "        if(bios[0] == 'B'):\n",
    "            #print(type_index)\n",
    "            new_type.append( dtypes[type_index])\n",
    "    \n",
    "    return ' '.join(sent), bios, new_type[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removebio(sent, bios, dtype, remove_set):\n",
    "    new_sent, new_bio, new_type = [], [], []\n",
    "    buffer = []\n",
    "    type_index = 0\n",
    "            \n",
    "    for index, (word, bio) in enumerate( zip(sent, bios)):\n",
    "        new_sent.append( word )\n",
    "\n",
    "        if( bio=='O'):\n",
    "            new_bio.append( bio )\n",
    "        else:\n",
    "            buffer.append(word)\n",
    "\n",
    "            if( bio == 'E'):\n",
    "                l = len(buffer)\n",
    "                new_type.append( dtype[type_index] )\n",
    "                type_index += 1\n",
    "                \n",
    "                for remove_len in sorted(sort_front_conj, reverse=True):\n",
    "                    if(len(buffer)<remove_len):\n",
    "                        continue\n",
    "                        \n",
    "                    remove_sents = sort_front_conj[remove_len]\n",
    "                    check_sent = ' '.join(buffer[:remove_len])\n",
    "                \n",
    "\n",
    "                    if(check_sent in remove_sents):\n",
    "                        # convert these data to others\n",
    "                        new_bio.extend( ['O']*(remove_len) )\n",
    "                        if((l-remove_len)>1):\n",
    "                            l = l-remove_len\n",
    "                            new_bio.append('B')\n",
    "                            if(l>=2):\n",
    "                                new_bio.extend( ['I']*(l-2) )\n",
    "                            new_bio.append('E')\n",
    "                        elif( (l-remove_len)==1 ):\n",
    "                            new_bio.append('O')\n",
    "                            new_type.pop()\n",
    "                        elif(l==remove_len):\n",
    "                            new_type.pop()\n",
    "                        else:\n",
    "                            raise ValueError(' should be shorter')\n",
    "                        break\n",
    "                else:    \n",
    "                    new_bio.append('B')\n",
    "                    if(l>=2):\n",
    "                        new_bio.extend( ['I']*(l-2) )\n",
    "                    new_bio.append('E')\n",
    "\n",
    "                buffer = []\n",
    "    assert(len(new_sent) == len(new_bio)),\"{}_{}\\n{}\\n{}\\n\".format(len(new_sent), len(new_bio), new_sent, new_bio)\n",
    "    return ' '.join(new_sent), new_bio, new_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "recover = copy.deepcopy(temp_recover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_conj = json.load(open('qq.json'))\n",
    "sort_front_conj = collections.defaultdict(list)\n",
    "\n",
    "for _ in front_conj:\n",
    "    if(_[0] in string.punctuation):\n",
    "        continue\n",
    "    l = len(_.strip().split())\n",
    "    sort_front_conj[l].append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for side in recover:\n",
    "    for index in range(len(recover[side]['sent'])):\n",
    "        temp = remove_fe_punctation(recover[side]['sent'][index].split().copy(), recover[side]['bio'][index].copy(), recover[side]['type'][index].copy())\n",
    "        temp = remove_last_conj(temp[0].split().copy(), temp[1].copy(), temp[2].copy())\n",
    "        temp = remove_fe_punctation(temp[0].split().copy(), temp[1].copy(), temp[2].copy())\n",
    "        temp = removebio(temp[0].split().copy(), temp[1].copy(), temp[2].copy(), sort_front_conj)\n",
    "        temp = remove_fe_punctation(temp[0].split().copy(), temp[1].copy(), temp[2].copy())\n",
    "        \n",
    "        recover[side]['sent'][index] = temp[0] \n",
    "        recover[side]['bio'][index]  = temp[1]\n",
    "        recover[side]['type'][index] = temp[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for side in recover:\n",
    "    for index in range(len(recover[side]['sent'])):\n",
    "        sent = recover[side]['sent'][index].split()\n",
    "        bios = recover[side]['bio'][index]\n",
    "        dtype = recover[side]['type'][index]\n",
    "        \n",
    "        c = [0, 0]\n",
    "        for word, bio in zip(sent, bios):\n",
    "            if(bio == 'B'):\n",
    "                c[0] += 1\n",
    "            if(bio == 'E'):\n",
    "                c[1] += 1\n",
    "        assert(c[0] == c[1])\n",
    "        assert(c[0] == len(dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stat = [collections.defaultdict(int), collections.defaultdict(int), collections.defaultdict(int), collections.defaultdict(int)]\n",
    "for side in recover:\n",
    "    for index in range(len(recover[side]['sent'])):\n",
    "        if(('< link >' in recover[side]['sent']) or ('< cite >' in recover[side]['sent']) or ('< user >' in recover[side]['sent']) or ('< reply >' in recover[side]['sent'])):\n",
    "            sent = recover[side]['sent'][index].split()\n",
    "            bios = recover[side]['bio'][index]\n",
    "            \n",
    "            temp_sent, temp_bio = [], []\n",
    "            word_index = 0\n",
    "            while(word_index<len(sent)):\n",
    "                temp_sent.append(sent[word_index])\n",
    "                temp_bio.append(bios[word_index])\n",
    "                for key in ['link', 'cite', 'reply', 'user']:\n",
    "                    if(sent[word_index:word_index+3]==['<', key, '>']):\n",
    "                        temp_sent.pop()\n",
    "                        temp_bio.pop()\n",
    "                        temp_sent.append('<{}>'.format(key))\n",
    "\n",
    "                        if(bios[word_index:word_index+3] == ['I', 'I', 'I']):\n",
    "                            temp_bio.append('I')\n",
    "                        elif(bios[word_index:word_index+3] == ['I', 'I', 'E']):\n",
    "                            temp_bio.append('E')\n",
    "                        elif(bios[word_index:word_index+3] == ['I', 'E', 'O']):\n",
    "                            temp_bio.append('E')\n",
    "\n",
    "                        elif(bios[word_index:word_index+3] == ['E', 'O', 'O']):\n",
    "                            temp_bio.append('E')\n",
    "\n",
    "                        elif(bios[word_index:word_index+3] == ['B', 'I', 'I']):\n",
    "                            temp_bio.append('B')\n",
    "                        elif(bios[word_index:word_index+3] == ['B', 'I', 'E']):\n",
    "                            temp_bio.append('O')\n",
    "\n",
    "                        elif(bios[word_index:word_index+3] == ['O', 'O', 'O']):\n",
    "                            temp_bio.append('O')\n",
    "                        elif(bios[word_index:word_index+3] == ['O', 'B', 'I']):\n",
    "                            temp_bio.append('B')\n",
    "                        word_index += 2\n",
    "                word_index += 1\n",
    "                \n",
    "            recover[side]['sent'][index] = ' '.join(temp_sent)\n",
    "            recover[side]['bio'][index] = temp_bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the clean post data and prepare paragraph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = {}\n",
    "for side, key in enumerate(['pos', 'neg']):\n",
    "    with open(base+'/parsing/clean_{}'.format(key)) as f:\n",
    "        f.readline()\n",
    "\n",
    "        for line in f:\n",
    "            clean_post = json.loads(line)\n",
    "            index, post_index, reply_index = [int(_) for _ in clean_post['uid'].split('_')]\n",
    "\n",
    "            if(index not in pairs):\n",
    "                pairs[index] = {'content':[[], []]}\n",
    "\n",
    "            while(len(pairs[index]['content'][side])<=post_index):\n",
    "                pairs[index]['content'][side].append([])\n",
    "\n",
    "            while(len(pairs[index]['content'][side][post_index])<=reply_index):\n",
    "                pairs[index]['content'][side][post_index].append(None)\n",
    "\n",
    "            pairs[index]['content'][side][post_index][reply_index] = {\n",
    "                'bio':[None for _ in clean_post['context']],\n",
    "                'context':clean_post['context'],\n",
    "                'author':clean_post['author'],\n",
    "                'type':[None for _ in clean_post['context']]\n",
    "            }\n",
    "\n",
    "with open(base+'/parsing/clean_op') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        clean_post = json.loads(line)    \n",
    "\n",
    "        index = int(clean_post['uid'])\n",
    "        pairs[index]['op_info'] = {\n",
    "                'bio':[None for _ in clean_post['context']],\n",
    "                'context':clean_post['context'],\n",
    "                'author':clean_post['author'],\n",
    "                'type':[None for _ in clean_post['context']],\n",
    "                'topic':clean_post['topic'],\n",
    "                'pos_author':clean_post['pos_author'],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "883"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for side, key in enumerate(['pos', 'neg']):\n",
    "    for index, sent, bio, dtype in zip(recover[key]['index'], recover[key]['sent'], recover[key]['bio'], recover[key]['type']):\n",
    "        index, post_index, reply_index, para_index = [ int(_) for _ in index_mapping[key][index].split('_')]\n",
    "\n",
    "        # bert tokenizer is different from nltk, thus need to use the new one\n",
    "        pairs[index]['content'][side][post_index][reply_index]['context'][para_index] = sent\n",
    "        pairs[index]['content'][side][post_index][reply_index]['bio'][para_index] = ' '.join(bio)\n",
    "        pairs[index]['content'][side][post_index][reply_index]['type'][para_index] = dtype\n",
    "\n",
    "key = 'op'\n",
    "for index, sent, bio, dtype in zip(recover[key]['index'], recover[key]['sent'], recover[key]['bio'], recover[key]['type']):\n",
    "    index, para_index = [ int(_) for _ in index_mapping[key][index].split('_')]\n",
    "\n",
    "    # bert tokenizer is different from nltk, thus need to use the new one\n",
    "    pairs[index]['op_info']['context'][para_index] = sent\n",
    "    pairs[index]['op_info']['bio'][para_index] = ' '.join(bio)\n",
    "    pairs[index]['op_info']['type'][para_index] = dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "883"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for index in pairs:\n",
    "    if((len(pairs[index]['content'][0])>0) and (len(pairs[index]['content'][1])>0)):\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 868)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concat(total_type):\n",
    "    dtype = []\n",
    "    for _ in total_type:\n",
    "        dtype.extend(_)\n",
    "    return dtype\n",
    "count = 0\n",
    "pair_index = list(pairs.keys())\n",
    "for index in pair_index:\n",
    "    try:\n",
    "        pairs[index]['op_info']['type'] = concat(pairs[index]['op_info']['type'])\n",
    "    except:\n",
    "        count += 1\n",
    "        del pairs[index]\n",
    "        continue\n",
    "    \n",
    "    for side in range(2):\n",
    "        temp_post = []\n",
    "        pos_authors = []\n",
    "        for post, pos_author in zip(pairs[index]['content'][side], pairs[index]['op_info']['pos_author'][side]):\n",
    "            temp_reply = []\n",
    "            \n",
    "            for reply in post:\n",
    "                try:\n",
    "                    temp_reply.append({\n",
    "                        'bio':reply['bio'],\n",
    "                        'context':reply['context'],\n",
    "                        'author':reply['author'],\n",
    "                        'type':concat(reply['type'])\n",
    "                    })\n",
    "                except TypeError:\n",
    "                    count += 1\n",
    "                    break\n",
    "            else:\n",
    "                pos_authors.append(pos_author)\n",
    "                temp_post.append(temp_reply)\n",
    "        \n",
    "        pairs[index]['op_info']['pos_author'][side] = pos_authors\n",
    "        pairs[index]['content'][side] = temp_post\n",
    "        if(len(pairs[index]['content'][side]) == 0):\n",
    "            del pairs[index]\n",
    "            break\n",
    "count, len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bio', 'context', 'author', 'type', 'topic', 'pos_author'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[index]['op_info'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert pairs into posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [[], []]\n",
    "for index in pairs:\n",
    "    for side in range(2):\n",
    "        op_info = {}\n",
    "        for key in ['bio', 'context', 'author', 'type']:\n",
    "            op_info[key] = pairs[index]['op_info'][key]\n",
    "            \n",
    "        for post_index, post in enumerate(pairs[index]['content'][side]):\n",
    "            temp = [copy.deepcopy(op_info)]\n",
    "            temp.extend(post)\n",
    "            \n",
    "            posts[side].append({\n",
    "                'content':temp,\n",
    "                'topic':pairs[index]['op_info']['topic'],\n",
    "                'uid':'{}_{}'.format(index, post_index),\n",
    "                'pos_author': pairs[index]['op_info']['pos_author'][side][post_index]\n",
    "            })\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save post\n",
    "for i, key in enumerate(['pos', 'neg']):\n",
    "    with open(base+'/tree/clean_post_{}.jsons'.format(key), 'w') as f:\n",
    "        for _ in posts[i]:\n",
    "            f.write(json.dumps(_))\n",
    "            f.write('\\n')\n",
    "# save post\n",
    "for i, key in enumerate(['pos', 'neg']):\n",
    "    with open(base+'/tree_bert/clean_post_{}.jsons'.format(key), 'w') as f:\n",
    "        for _ in posts[i]:\n",
    "            f.write(json.dumps(_))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4293 6078\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for side in range(2):\n",
    "    for post in posts[side]:\n",
    "        l = 0\n",
    "        for _ in post['content']:\n",
    "            l += len(_['type'])\n",
    "        if(l<100):\n",
    "            c+=1\n",
    "print(c, 2*len(posts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## elmo tree version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load posts if needed\n",
    "posts = []\n",
    "for i, key in enumerate(['pos', 'neg']):\n",
    "    with open(base+'/tree/clean_post_{}.jsons'.format(key)) as f:\n",
    "        posts.append([json.loads(line) for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(26,25,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process link \n",
    "c = []\n",
    "def find_shell(index, last, context, adu_index):\n",
    "    #print(last, index)\n",
    "    if(index==0):\n",
    "        return (adu_index, 0, 0)\n",
    "    for i in range(index, last, -1):\n",
    "        if(context[i] in [\".\", \"!\", \"?\", \"</ac>\", \"<para>\"]):\n",
    "            if(i>=index):\n",
    "                return (adu_index, index, index)\n",
    "            else:\n",
    "                return (adu_index, i+1, index)\n",
    "    \n",
    "    return (adu_index, index, index)\n",
    "\n",
    "\n",
    "def prepare(data, elmo_preprocess):\n",
    "    mask = []\n",
    "    shell_span, span, elmo_index = [], [], []\n",
    "    adu_label = []\n",
    "    author = []\n",
    "    para_author = []\n",
    "    \n",
    "    topic_elmo_index = len(elmo_preprocess)\n",
    "    elmo_preprocess.append('<topic> '+data['topic']+' </topic>')\n",
    "    elmo_preprocess[-1] = ' '.join(elmo_preprocess[-1].split())\n",
    "    \n",
    "    ac_position_info = []\n",
    "    adu_index = 0\n",
    "    for post_pos, post in enumerate(data['content']):\n",
    "        for bio, context in zip(post['bio'], post['context']):\n",
    "            elmo_index.append(len(elmo_preprocess))\n",
    "\n",
    "            elmo_preprocess.append(['<para>'])\n",
    "            bios, sent = bio.split(), context.split()\n",
    "\n",
    "            last = -1\n",
    "            for bio, word in zip(bios, sent):\n",
    "                if(bio == 'B'):\n",
    "                    span.append([adu_index, len(elmo_preprocess[-1]), 0])\n",
    "                    shell_span.append(find_shell(len(elmo_preprocess[-1])-1, last, elmo_preprocess[-1], adu_index))\n",
    "                    elmo_preprocess[-1].append('<ac>')\n",
    "                    \n",
    "                elmo_preprocess[-1].append(word)\n",
    "                \n",
    "                if(bio == 'E'):\n",
    "                    span[-1][-1] = len(elmo_preprocess[-1])\n",
    "                    last = span[-1][-1]\n",
    "                    elmo_preprocess[-1].append('</ac>')\n",
    "                    \n",
    "            elmo_preprocess[-1].append('</para>')\n",
    "            elmo_preprocess[-1] = ' '.join(elmo_preprocess[-1])\n",
    "            # update parameter\n",
    "            adu_index += 1\n",
    "        # build mask\n",
    "        dtype = post['type']\n",
    "        prev_len, now_len = len(adu_label), len(dtype)\n",
    "        for adu_pos, _ in enumerate(dtype):\n",
    "            if(_ == 'P'):\n",
    "                # constrain search space to this reply\n",
    "                mask.append([0]*prev_len + [1]*now_len)\n",
    "\n",
    "            elif(_ == 'C'):\n",
    "                # constrain search space to all\n",
    "                mask.append([1]*(prev_len + now_len))\n",
    "\n",
    "            ac_position_info.append([adu_pos, post_pos])\n",
    "\n",
    "        adu_label.extend( dtype )\n",
    "\n",
    "    if(('[deleted]' in data['pos_author'][0]) and data['content'][-1]['author'] == '[deleted]'):\n",
    "        for index, post in enumerate(data['content']):\n",
    "            author.extend([1+index&1]*len(post['type']))\n",
    "            para_author.extend([1+index&1]*len(post['bio']))\n",
    "    else:\n",
    "        for post in data['content']:\n",
    "            if(post['author'] in data['pos_author'][0]):\n",
    "                author.extend([1]*len(post['type']))\n",
    "                para_author.extend([1]*len(post['bio']))\n",
    "            elif(post['author'] in data['pos_author'][1]):\n",
    "                author.extend([2]*len(post['type']))\n",
    "                para_author.extend([2]*len(post['bio']))\n",
    "            else:\n",
    "                author.extend([0]*len(post['type']))\n",
    "                para_author.extend([0]*len(post['bio']))\n",
    "\n",
    "    adu_map = {'P':0, 'C':1}\n",
    "    adu_label = [ adu_map[_] for _ in adu_label]\n",
    "        \n",
    "    return {\n",
    "        'topic_index':topic_elmo_index, \n",
    "        'elmo_index':elmo_index,\n",
    "        'shell_span':shell_span, \n",
    "        'span':span, \n",
    "        'mask':mask, \n",
    "        'author':author,\n",
    "        'para_author':para_author,\n",
    "        'adu_label':adu_label,\n",
    "        'ac_position_info':ac_position_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prepare_data = [[], []]\n",
    "elmo_preprocess = [[], []]\n",
    "\n",
    "for side in range(2):\n",
    "    for _ in posts[side]:\n",
    "        temp = prepare(_, elmo_preprocess[side])\n",
    "        temp['uid'] = _['uid']\n",
    "        prepare_data[side].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': [{'bio': ['B I I I I I I I I I I I I I I I I I E O',\n",
       "    'B I I I I I I I I I I I I I I I I I I I I I I I I I E O',\n",
       "    'B I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I E O O O O B I I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I I E O B I I I I I I E O',\n",
       "    'B I I I I I I I I I I I I I I I I I I I I I I I I I E O O B I I I I I I I I I I I I I I I I I E O O O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I I I E O',\n",
       "    'O O O',\n",
       "    'O'],\n",
       "   'context': ['this is not meant as a disrespect to most organized religions , merely an observation i have come to .',\n",
       "    'the widespread belief or following of most organized religions ( christianity , islam , judaism ) is a sign of human weakness in a couple of ways .',\n",
       "    'the need for an afterlife in most religions satisfies our general fears of death , and the impermanence and futility of our lives . the theory is not backed by much scientific evidence as far as i know , so the reason to believe in an afterlife is not that it makes more sense but that it makes life easier . a fear of death and impermanence so strong that one must believe in something that i would categorize as fairytale . this is what i would call a weakness .',\n",
       "    'the need for moral guidance in life - to need guidance from religion to know the difference between right and wrong is also a sign of weakness in that it shows a lack of judgement and wisdom for one to decide for themselves what is right or wrong . furthermore , the need of a consequence by eternal damnation as persuasion not to do \" bad things \" , and the need of an incentive by eternal salvation to do good . is a sign of weakness in that it shows that human - nature is bad , or barbaric in a sense .',\n",
       "    '< edit >',\n",
       "    '<cite>'],\n",
       "   'author': 'flyinhomekites',\n",
       "   'type': ['C', 'C', 'P', 'P', 'P', 'P', 'C', 'P', 'P', 'P', 'C']},\n",
       "  {'bio': ['O O O O O O O O O O B I I I I I I I I I I I I I I E O B I I I I I I I I I I I I E O B I I I I E O B I I E O O B I I I E O B I I I I I I I I I I I I I I I E O B I I I E O',\n",
       "    'O B I I I I I I I E O O B I I E O',\n",
       "    'B I I I I I I I E O O B I I I I I I I I I I I I I I I I I E O B I I I I I I I I I E O O O B I I I I I I I I I I I I I I I E O'],\n",
       "   'context': [\"<cite> you ' re reversing cause and effect here . one of the motivators of religious thought is the view that certain things are morally true . it ' s wrong to kill , cheat , steal , lie , etc . people wondered why this was true . these are intelligible laws , so they must come from intelligence . they are innate in nature , and thus the intelligence that made them made all of nature . they call this intelligence god .\",\n",
       "    \"obviously that ' s a great simplification of complex theology , but you get the point .\",\n",
       "    \"you can disagree all you want about those conclusions , but the fact remains that deciding for one ' s self what is just does not make something actually just . something being objectively right or wrong can only come from god . hence , a belief in moral principles translated into a belief in god , not the other way around .\"],\n",
       "   'author': 'FuzzyCheese',\n",
       "   'type': ['P', 'P', 'P', 'P', 'P', 'P', 'C', 'C', 'C', 'P', 'P', 'P', 'P']},\n",
       "  {'bio': ['O O O O B I I I I I I I I I I I I I I I E O O O O O O O O O O O O O O O O O O O O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I E O O O O O O O',\n",
       "    'O O O O O O O B I I I I I I I I I E O'],\n",
       "   'context': ['while i disagree that those things are \" objectively \" wrong , or that they are necessarily \" intelligible laws \" . i see no way to argue with that reasoning . of course i disagree with the conclusions though . i don \\' t know i could say for sure that the belief in every deity stemmed solely from this , but as at least being one of the major reasonings . i can agree with that .',\n",
       "    'you changed my view there . although i have not yet found adequate reasonings on my other points .'],\n",
       "   'author': 'flyinhomekites',\n",
       "   'type': ['P', 'C', 'C']},\n",
       "  {'bio': ['O O O O B I E B I I I I I E O',\n",
       "    'O O O O O O O B I I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I E O O B I I I I I I I I I I I I I I I I I I I I I I I I I I I E O',\n",
       "    'O O B I I I I I I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I I I I E O',\n",
       "    'O B I I I E O O B I I I I I I I E O'],\n",
       "   'context': ['well i think for your other points it depends on what you consider weakness .',\n",
       "    \"but regardless of that , say , back 3000 years ago , that you followed the same thought process i outlined and came to the conclusion that god was a reality . wouldn ' t the logical next step be an afterlife ? if you believed god put in place rules that all of mankind must follow , wouldn ' t there be a reason to postulate an effect of following those rules ?\",\n",
       "    \"i think you ' re confusing ancient philosophy , which was indeed ignorant of the way much of the world worked , with human weakness , which it is not . it ' s just an attempt to explain things only humans can perceive .\",\n",
       "    \"obviously this depends on the religion , but i ' m just talking about general monotheism here .\"],\n",
       "   'author': 'FuzzyCheese',\n",
       "   'type': ['C', 'C', 'P', 'C', 'C', 'P', 'P', 'C', 'C']},\n",
       "  {'bio': ['O O O O O O B I I I I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I E O O B I I I I I I I I I I I I I I I I I I I I I I I I I I I E O O',\n",
       "    'O O O O O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I I I I I I I I E O'],\n",
       "   'context': ['\" but regardless of that , say , back 3000 years ago , that you followed the same thought process i outlined and came to the conclusion that god was a reality . wouldn \\' t the logical next step be an afterlife ? if you believed god put in place rules that all of mankind must follow , wouldn \\' t there be a reason to postulate an effect of following those rules ? \"',\n",
       "    'yes , logically speaking . our continued belief of this in modern times seems like less of a logical necessity to me , and more of a way to satiate our fears about the ends of our lives . i only use ancient philosophy as an example of how \" magical thinking \" ensues in light of a general ignorance about the world around us .'],\n",
       "   'author': 'flyinhomekites',\n",
       "   'type': ['P', 'C', 'C', 'P', 'C']},\n",
       "  {'bio': ['B E O B I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I I I I I I I I I I E O',\n",
       "    'O O O B I I I I I I I I I I I I I I E O B I I I I I E O O B I I I I I I I I I I I I I I I I I I I I E O B I I E O B I I I I I I I I I I E O',\n",
       "    'B I I I I I I I I I I I I I I I I I I I I I I E O O B I I I I I E O'],\n",
       "   'context': [\"how come ? i don ' t really see anything that important to the reasoning i ' ve been talking about as having changed . the belief that morals are objective has not been proven or dis - proven , and is just as rational or irrational as it ' s ever been .\",\n",
       "    \"but regardless , the cause of religious faith is not what you ' re making it out to be . at least not for seriously religious people . if you go to a seminary / theological school , the people there aren ' t convinced out of their fear of death . they ' re there because the way they see the world led them to believe in god .\",\n",
       "    \"you can say it ' s a sign of human ignorance or whatever else you wan na say if you think religion is wrong , but it certainly is not out of weakness .\"],\n",
       "   'author': 'FuzzyCheese',\n",
       "   'type': ['P', 'C', 'C', 'P', 'P', 'P', 'C', 'C', 'C', 'C']},\n",
       "  {'bio': ['B I I I I I I E O O O O B I I I I I I I I I I I I I I I E O',\n",
       "    'B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I E O',\n",
       "    'O O B I E O O B I I I I I I E O B I I I I I I I I I E O O O O'],\n",
       "   'context': [\"i don ' t they ever will be . . . if there is a creator though , according to modern religions , then there are objective moral values .\",\n",
       "    \"i don ' t think anyone would explicitly say the sole purpose of my belief in my religion is to satiate my fears of death , impermanence , futility , and the meaningless of existence and morals .\",\n",
       "    \"i think religion is wrong , but it ' s possible that it is right . both assumptions are under a vail of ignorance about the universe , either way .\"],\n",
       "   'author': 'flyinhomekites',\n",
       "   'type': ['C', 'C', 'C', 'C', 'C', 'C']},\n",
       "  {'bio': ['O O O O O O O B I I I I E O B I I I I I E O B I I I I I I I E O B I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I E O',\n",
       "    'O O O O O O O O B I I I I I I I I I I I I E O'],\n",
       "   'context': [\"<cite> well of course not . because no one is actually like that . take thomas aquinas ' five ways <link> . these are rational arguments for the existence of god . the irrational , emotional side of religion is a relatively new thing mostly found in american protestantism . go to orthodoxy and catholicism and you see scholarly , academic reasoning for doctrine .\",\n",
       "    '<cite> yeah , but can you see that religion is not a position of weakness , just of differing perspectives and reasoning ?'],\n",
       "   'author': 'FuzzyCheese',\n",
       "   'type': ['P', 'P', 'P', 'P', 'C', 'C']}],\n",
       " 'topic': 'the widespread belief of religion is a sign of human weakness',\n",
       " 'uid': '153_2',\n",
       " 'pos_author': [['flyinhomekites'], ['FuzzyCheese']]}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<para> <ac> the vice president often is called to act as a replacement for a duty or appearance if the president is not around </ac> . for instance , <ac> meeting foreign dignitaries if the president is out of town </ac> . <ac> there needs to be a second face of the executive branch </ac> . </para>\n",
      "\n",
      "for instance ,\n"
     ]
    }
   ],
   "source": [
    "index, start, end = 4, 26, 28\n",
    "index = prepare_data[0][0]['elmo_index'][index]\n",
    "print(elmo_preprocess[0][index])\n",
    "print()\n",
    "print(' '.join(elmo_preprocess[0][index].split()[start:end+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_preprocess[0][prepare_data[0][0]['elmo_index'][1]].split()[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 5) [0, 6, 22]\n",
      "(0, 23, 23) [0, 24, 34]\n",
      "(0, 36, 37) [0, 38, 56]\n",
      "(1, 0, 0) [1, 1, 25]\n",
      "(1, 26, 26) [1, 27, 42]\n",
      "(1, 43, 43) [1, 44, 52]\n",
      "(1, 53, 53) [1, 54, 76]\n",
      "(1, 78, 78) [1, 79, 99]\n",
      "(1, 101, 107) [1, 108, 132]\n",
      "(2, 1, 5) [2, 6, 31]\n",
      "(2, 32, 32) [2, 33, 54]\n",
      "(3, 0, 0) [3, 1, 38]\n",
      "(3, 40, 40) [3, 41, 54]\n",
      "(3, 68, 69) [3, 70, 99]\n",
      "(3, 101, 102) [3, 103, 130]\n",
      "(3, 132, 133) [3, 134, 149]\n",
      "(3, 150, 150) [3, 151, 185]\n",
      "(4, 0, 0) [4, 1, 24]\n",
      "(4, 26, 28) [4, 29, 40]\n",
      "(4, 41, 41) [4, 42, 54]\n",
      "(5, 1, 3) [5, 4, 45]\n",
      "(5, 46, 46) [5, 47, 60]\n",
      "(6, 1, 15) [6, 16, 34]\n",
      "(6, 35, 35) [6, 36, 56]\n",
      "(6, 57, 58) [6, 59, 68]\n",
      "(6, 70, 81) [6, 82, 104]\n",
      "(6, 106, 109) [6, 110, 140]\n",
      "(7, 0, 0) [7, 1, 12]\n",
      "(7, 13, 13) [7, 14, 32]\n",
      "(7, 33, 33) [7, 34, 67]\n",
      "(7, 68, 68) [7, 69, 103]\n",
      "(7, 105, 110) [7, 111, 136]\n",
      "(7, 137, 137) [7, 138, 163]\n",
      "(7, 165, 167) [7, 168, 192]\n",
      "(7, 193, 193) [7, 194, 204]\n",
      "(7, 205, 208) [7, 209, 222]\n",
      "(8, 1, 2) [8, 3, 20]\n",
      "(8, 21, 21) [8, 22, 48]\n",
      "(8, 49, 49) [8, 50, 60]\n"
     ]
    }
   ],
   "source": [
    "side, index = 0, 0\n",
    "for a, b in zip(prepare_data[side][index]['shell_span'], prepare_data[side][index]['span']):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': [{'bio': ['O O O O O B I I I I I I I I I I I I I E O B I I I I I I I E O O O B I I I I I I I I I I I I I I I E O',\n",
       "    'B I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I E O B I I I I I E O B I I I I I I I I I I I I I I I I I I I E O O B I I I I I I I I I I I I I I I I I E O O O O O O O O B I I I I I I I I I I I I I I I I I I I I I E O',\n",
       "    'O O O O O B I I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I I E O',\n",
       "    'B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I E O O B I I I I I I I I I I E O O O O O O O O O O O O O O O B I I I I I I I I I I I I I I I I I I I I I I I I I I E O O O B I I I I I I I I I I I I I I I I I I I I I I I I E O O O B I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I E O'],\n",
       "   'context': ['from what i understand , the only significant duty given to the vice president is to preside over the senate ; even this has been shirked by past vice presidents . furthermore , the ability to cast a tie breaking vote has not been invoked very often by vice presidents .',\n",
       "    \"in the event that the president is either killed or resigns , the vice president is a horrible choice to take over office . the speaker of the house would be much more qualified for the position simply because they engage more deeply with the government . the vice president is not significantly involved with congress and does not engage in debate with representatives or senators over legislation . furthermore they do not command the same degree of loyalty and respect as the president or speaker of the house . i ' m willing to bet that john boehner would have an easier time dealing with congress as president than joe biden would due to his constant interaction with it .\",\n",
       "    'as an example , if obama was assassinated in late 2009 , would biden have the same level of influence necessary to gather support for the affordable care act ? does biden have the same level of respect from foreign nations needed to guide the country in this global age ?',\n",
       "    'the only half decent argument i can think of for a vice president is to ensure that the president \\' s general will / ideology is carried out in the event of their removal from office . if boehner suddenly took office he would absolutely veto many bills obama supported . i believe that this is a weak argument for two reasons . first , the speaker of the house does represent a significant portion of the country \\' s will , given that their party has taken the majority in the house . yes , there may be conflict between the old cabinet members and the new president but the position is still somewhat representative of the country \\' s will . secondly , the vice president is not guaranteed to adhere to the policies of their predecessor . in robert mcnamara \\' s documentary , \" the fog of war , \" he mentions how lbj decided to continue the vietnam war despite jfk \\' s efforts to move troops out .'],\n",
       "   'author': 'TentacleBird',\n",
       "   'type': ['P',\n",
       "    'C',\n",
       "    'C',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'C',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'C']},\n",
       "  {'bio': ['B I I I I I I I I I I I I I I I I I I I I E O O O O B I I I I I I I I E O B I I I I I I I I I E O',\n",
       "    'O O O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I I I E O'],\n",
       "   'context': ['the vice president often is called to act as a replacement for a duty or appearance if the president is not around . for instance , meeting foreign dignitaries if the president is out of town . there needs to be a second face of the executive branch .',\n",
       "    'also , if the second in succession to the presidency was the speaker of the house , the opposing party might take it upon themselves to try and assassinate or impeach the president simply for the reason of taking over the executive branch . the position of vice president exists as a check and balance position .'],\n",
       "   'author': 'draculabakula',\n",
       "   'type': ['P', 'P', 'C', 'P', 'P']},\n",
       "  {'bio': ['O O O O O O O O O O O O O O O B I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I E O O B I I I I I I E O O O O O O O O O O O O O B I I I I I I I I I I I I I I I I I I I E O O O O O B I I I I I I I I I I I I I I I I I I I I I I I I I I I E O',\n",
       "    'B I I I I I I I I E O B I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I E O O O O O O O B I I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I I I I I I E O O O O B I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I E O O O O B I I I I I I I I I I E O'],\n",
       "   'context': ['your first point is very strong , however , nothing leads me to believe that the second face of the executive branch couldn \\' t be held by a close cabinet member . the secretary of state , for example , would be a much more powerful presence than the vice president given that they have far more impact on global affairs . my counterpoint is not perfect though , since i do realize that cabinet members may not have the time to entertain guests and that the vice president has a much more flexible schedule . i would propose that the title of \" vice president \" be kept as well as their duty to meet with foreign dignitaries , but to remove their right to succession for presidency .',\n",
       "    'your second point has me hinging on awarding a delta . you are correct that the vice president is a strong buffer / barrier to radical political change ; the president and vice president are purposely kept in separate locations and have their own secret service , making it much harder to put a radically different leader in power through assassination . i also remember reading that andrew johnson was supposed to be assassinated alongside lincoln but that the plan failed ( meaning that it is much harder to kill 2 people than 1 ) . despite this , i believe that given modern day security as well as a free media to expose conspiracies , usurpation of power through assassination would not be a problem . there was a til a while ago about a general who blew the whistle on some corporate leaders who wanted to overthrow the government . i believe that america is not in a state where the members of the government are willing to resort to killing one another to seize power . this is absolutely not the case for all countries , but i believe it is not enough of a danger in the u . s .'],\n",
       "   'author': 'TentacleBird',\n",
       "   'type': ['P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'C',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P',\n",
       "    'P']},\n",
       "  {'bio': ['O O B I I I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I I I I I I I E O B I I I I I I I E O'],\n",
       "   'context': ['i think americans in general have become far to over comfortable in the security of our elected officials . it was very recently that someone climbed the gate in the white house and got all the way to the oval office before being caught . reagan was lucky to survive when he was shot .'],\n",
       "   'author': 'draculabakula',\n",
       "   'type': ['P', 'P', 'C']}],\n",
       " 'topic': 'the position of vice president of the united states should be eliminated from our government.',\n",
       " 'uid': '0_0',\n",
       " 'pos_author': [['TentacleBird'], ['draculabakula']]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for side in range(2):\n",
    "    for i, post in enumerate(prepare_data[side]):\n",
    "        for _ in post['elmo_index']:\n",
    "            assert(_<len(elmo_preprocess[side])),'{} {}'.format(side, i)\n",
    "        assert(post['topic_index']<len(elmo_preprocess[side])),'{} {}'.format(side, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for side, key in enumerate(['pos', 'neg']):\n",
    "    with open('./../preprocess/cmv_raw_v2/tree/cmv_elmo_{}.txt'.format(key), 'w') as f:\n",
    "        for line in elmo_preprocess[side]:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(['pos', 'neg']):\n",
    "    with open('./../preprocess/cmv_raw_v2/tree/pre_{}.jsons'.format(key), 'w') as f:\n",
    "        for _ in prepare_data[i]:\n",
    "            f.write(json.dumps(_))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8\n",
      "1 106\n",
      "1 187\n",
      "1 189\n",
      "1 192\n",
      "1 209\n",
      "1 232\n",
      "1 242\n",
      "1 328\n",
      "1 385\n",
      "1 388\n",
      "1 422\n",
      "1 426\n",
      "1 430\n",
      "1 452\n",
      "1 510\n",
      "1 523\n",
      "1 563\n",
      "1 575\n",
      "1 581\n",
      "1 600\n",
      "1 621\n",
      "1 649\n",
      "1 660\n",
      "1 696\n",
      "1 710\n",
      "1 791\n",
      "1 801\n",
      "1 907\n",
      "1 908\n",
      "1 917\n",
      "1 952\n",
      "1 959\n",
      "1 997\n",
      "1 1042\n",
      "1 1144\n",
      "1 1185\n",
      "1 1187\n",
      "1 1189\n",
      "1 1197\n",
      "1 1208\n",
      "1 1213\n",
      "1 1225\n",
      "1 1231\n",
      "1 1247\n",
      "1 1262\n",
      "1 1297\n",
      "1 1316\n",
      "1 1331\n"
     ]
    }
   ],
   "source": [
    "for side in range(2):\n",
    "    for index, _ in enumerate(prepare_data[side]):\n",
    "        if(0 in _['author']):\n",
    "            print(side, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['MicrosizeMe', 'gmoneygangster3'], ['DrinkyDrank', 'DangerouslyUnstable']]\n",
      "MicrosizeMe 4 7 DrinkyDrank 1 2 scottevil110 3 4 mrgoodnighthairdo 1 1 DangerouslyUnstable 1 2 mrgoodnighthairdo 1 1 DangerouslyUnstable 2 8 ProfThadBach 1 0 DangerouslyUnstable 2 8 \n",
      "[1, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[1, 1, 1, 1, 2, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "side, index = 1, 106\n",
    "print(posts[side][index]['pos_author'])\n",
    "for _ in posts[side][index]['content']:\n",
    "    print(_['author'], len(_['bio']), len(_['type']), end=' ')\n",
    "temp = prepare(posts[side][index], elmo_preprocess[side])\n",
    "print()\n",
    "print(temp['author'])\n",
    "print(temp['para_author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/nfs/nas-5.1/kyhuang/preprocess/cmv_raw_origin_v4/train//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data, posts = [], []\n",
    "for i, key in enumerate(['pos', 'neg']):\n",
    "    with open(base+'/tree/pre_{}.jsons'.format(key)) as f:\n",
    "        prepare_data.append([json.loads(_) for _ in f])\n",
    "    with open(base+'/tree/clean_post_{}.jsons'.format(key)) as f:\n",
    "        posts.append([json.loads(line) for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_preprocess = []\n",
    "for side, key in enumerate(['pos', 'neg']):\n",
    "    with open(base+'/tree/cmv_elmo_{}.txt'.format(key)) as f:\n",
    "        elmo_preprocess.append([line for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'topic', 'uid', 'pos_author'])\n",
      "dict_keys(['bio', 'context', 'author', 'type'])\n"
     ]
    }
   ],
   "source": [
    "print(posts[0][0].keys())\n",
    "print(posts[0][0]['content'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<ac>',\n",
       " 'i',\n",
       " \"'\",\n",
       " 've',\n",
       " 'seen',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'posts',\n",
       " 'condemning',\n",
       " 'islam',\n",
       " 'as',\n",
       " 'a',\n",
       " 'violent',\n",
       " 'religion',\n",
       " 'or',\n",
       " 'a',\n",
       " 'sexist',\n",
       " 'religion',\n",
       " '</ac>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_preprocess[0][1].split()[3:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic_index': 0,\n",
       " 'elmo_index': [1, 2, 3, 4, 5, 6, 7],\n",
       " 'shell_span': [[0, 0, 2],\n",
       "  [0, 23, 27],\n",
       "  [0, 60, 63],\n",
       "  [1, 0, 7],\n",
       "  [1, 16, 17],\n",
       "  [1, 40, 43],\n",
       "  [3, 0, 0],\n",
       "  [3, 18, 19],\n",
       "  [3, 25, 27],\n",
       "  [4, 0, 3],\n",
       "  [4, 27, 29],\n",
       "  [5, 0, 4],\n",
       "  [6, 0, 0],\n",
       "  [6, 17, 20],\n",
       "  [6, 31, 33],\n",
       "  [6, 44, 45]],\n",
       " 'span': [[0, 3, 22],\n",
       "  [0, 28, 59],\n",
       "  [0, 64, 68],\n",
       "  [1, 8, 16],\n",
       "  [1, 18, 39],\n",
       "  [1, 44, 68],\n",
       "  [3, 1, 17],\n",
       "  [3, 20, 25],\n",
       "  [3, 28, 36],\n",
       "  [4, 4, 26],\n",
       "  [4, 30, 48],\n",
       "  [5, 5, 31],\n",
       "  [6, 1, 16],\n",
       "  [6, 21, 30],\n",
       "  [6, 34, 44],\n",
       "  [6, 46, 51]],\n",
       " 'mask': [[1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]],\n",
       " 'author': [1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2],\n",
       " 'para_author': [1, 1, 1, 2, 1, 1, 2],\n",
       " 'adu_label': [0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0],\n",
       " 'ac_position_info': [[0, 0],\n",
       "  [1, 0],\n",
       "  [2, 0],\n",
       "  [3, 0],\n",
       "  [4, 0],\n",
       "  [5, 0],\n",
       "  [0, 1],\n",
       "  [1, 1],\n",
       "  [2, 1],\n",
       "  [0, 2],\n",
       "  [1, 2],\n",
       "  [2, 2],\n",
       "  [0, 3],\n",
       "  [1, 3],\n",
       "  [2, 3],\n",
       "  [3, 3]],\n",
       " 'uid': '0_0'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = [[], []]\n",
    "for side in range(2):\n",
    "    for index, data in enumerate(posts[side]):\n",
    "        temp = data['content'][1]['author']\n",
    "        for _ in data['content']:\n",
    "            if(_['author'] not in data['pos_author'] and _['author']!=temp):\n",
    "                check[side].append(index)\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1119, 5635)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check[1]), len(posts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 1370)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check[1]), len(posts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quietandproud\n",
      "clairebones\n"
     ]
    }
   ],
   "source": [
    "for _ in data['content']:\n",
    "    print(_['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(int,\n",
       "             {4: 1268,\n",
       "              10: 169,\n",
       "              2: 1002,\n",
       "              6: 424,\n",
       "              8: 169,\n",
       "              7: 3,\n",
       "              9: 1,\n",
       "              5: 2,\n",
       "              3: 1}),\n",
       " defaultdict(int, {4: 321, 2: 788, 6: 148, 8: 69, 3: 15, 9: 4, 5: 17, 7: 8})]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = [collections.defaultdict(int), collections.defaultdict(int)]\n",
    "for side in range(2):\n",
    "    for data in posts[side]:\n",
    "        count[side][len(data['content'])] +=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elmo_index': [defaultdict(int,\n",
       "              {9: 189,\n",
       "               17: 115,\n",
       "               7: 187,\n",
       "               20: 76,\n",
       "               8: 170,\n",
       "               5: 117,\n",
       "               3: 34,\n",
       "               18: 113,\n",
       "               25: 39,\n",
       "               23: 56,\n",
       "               16: 130,\n",
       "               15: 118,\n",
       "               22: 62,\n",
       "               11: 170,\n",
       "               12: 171,\n",
       "               6: 131,\n",
       "               14: 154,\n",
       "               13: 157,\n",
       "               21: 72,\n",
       "               19: 101,\n",
       "               10: 174,\n",
       "               62: 2,\n",
       "               36: 16,\n",
       "               31: 39,\n",
       "               47: 4,\n",
       "               45: 5,\n",
       "               24: 52,\n",
       "               29: 26,\n",
       "               26: 50,\n",
       "               30: 26,\n",
       "               34: 19,\n",
       "               4: 60,\n",
       "               28: 25,\n",
       "               2: 16,\n",
       "               33: 16,\n",
       "               51: 2,\n",
       "               38: 7,\n",
       "               32: 17,\n",
       "               37: 10,\n",
       "               35: 18,\n",
       "               40: 8,\n",
       "               27: 34,\n",
       "               64: 1,\n",
       "               41: 6,\n",
       "               43: 4,\n",
       "               72: 1,\n",
       "               53: 4,\n",
       "               46: 4,\n",
       "               39: 10,\n",
       "               52: 3,\n",
       "               57: 1,\n",
       "               59: 1,\n",
       "               42: 3,\n",
       "               54: 1,\n",
       "               48: 2,\n",
       "               92: 1,\n",
       "               58: 1,\n",
       "               44: 3,\n",
       "               61: 3,\n",
       "               60: 1,\n",
       "               73: 1}),\n",
       "  defaultdict(int,\n",
       "              {9: 92,\n",
       "               7: 91,\n",
       "               13: 77,\n",
       "               15: 46,\n",
       "               32: 15,\n",
       "               22: 32,\n",
       "               16: 66,\n",
       "               18: 52,\n",
       "               3: 13,\n",
       "               5: 59,\n",
       "               19: 36,\n",
       "               23: 26,\n",
       "               12: 96,\n",
       "               6: 75,\n",
       "               11: 75,\n",
       "               24: 15,\n",
       "               8: 87,\n",
       "               17: 45,\n",
       "               10: 83,\n",
       "               4: 32,\n",
       "               14: 51,\n",
       "               64: 1,\n",
       "               34: 5,\n",
       "               21: 32,\n",
       "               20: 33,\n",
       "               29: 17,\n",
       "               27: 21,\n",
       "               40: 3,\n",
       "               31: 5,\n",
       "               35: 5,\n",
       "               26: 16,\n",
       "               25: 16,\n",
       "               43: 2,\n",
       "               41: 3,\n",
       "               28: 14,\n",
       "               53: 1,\n",
       "               45: 2,\n",
       "               56: 1,\n",
       "               38: 3,\n",
       "               49: 2,\n",
       "               39: 1,\n",
       "               30: 10,\n",
       "               33: 4,\n",
       "               36: 1,\n",
       "               37: 1,\n",
       "               42: 2,\n",
       "               54: 1,\n",
       "               47: 2,\n",
       "               50: 1,\n",
       "               44: 1})],\n",
       " 'shell_span': [defaultdict(int,\n",
       "              {39: 45,\n",
       "               35: 59,\n",
       "               11: 45,\n",
       "               44: 41,\n",
       "               29: 78,\n",
       "               14: 60,\n",
       "               7: 21,\n",
       "               9: 26,\n",
       "               6: 9,\n",
       "               52: 28,\n",
       "               65: 11,\n",
       "               54: 24,\n",
       "               87: 4,\n",
       "               72: 9,\n",
       "               20: 86,\n",
       "               23: 74,\n",
       "               43: 33,\n",
       "               45: 42,\n",
       "               48: 37,\n",
       "               56: 29,\n",
       "               47: 26,\n",
       "               63: 22,\n",
       "               46: 36,\n",
       "               60: 15,\n",
       "               40: 46,\n",
       "               13: 59,\n",
       "               16: 68,\n",
       "               15: 61,\n",
       "               25: 59,\n",
       "               32: 81,\n",
       "               27: 79,\n",
       "               64: 17,\n",
       "               71: 11,\n",
       "               33: 54,\n",
       "               30: 72,\n",
       "               36: 57,\n",
       "               41: 41,\n",
       "               49: 34,\n",
       "               26: 72,\n",
       "               22: 83,\n",
       "               73: 6,\n",
       "               42: 42,\n",
       "               75: 6,\n",
       "               19: 78,\n",
       "               38: 50,\n",
       "               28: 72,\n",
       "               17: 78,\n",
       "               21: 90,\n",
       "               24: 66,\n",
       "               59: 23,\n",
       "               51: 37,\n",
       "               189: 1,\n",
       "               66: 10,\n",
       "               77: 10,\n",
       "               80: 6,\n",
       "               69: 11,\n",
       "               12: 57,\n",
       "               31: 55,\n",
       "               57: 22,\n",
       "               50: 18,\n",
       "               88: 3,\n",
       "               18: 76,\n",
       "               8: 19,\n",
       "               53: 24,\n",
       "               37: 45,\n",
       "               58: 18,\n",
       "               55: 18,\n",
       "               107: 3,\n",
       "               86: 8,\n",
       "               67: 9,\n",
       "               123: 2,\n",
       "               119: 1,\n",
       "               61: 19,\n",
       "               134: 2,\n",
       "               195: 1,\n",
       "               4: 5,\n",
       "               34: 57,\n",
       "               79: 3,\n",
       "               81: 4,\n",
       "               89: 3,\n",
       "               95: 5,\n",
       "               76: 10,\n",
       "               5: 8,\n",
       "               93: 2,\n",
       "               78: 7,\n",
       "               62: 20,\n",
       "               118: 3,\n",
       "               109: 4,\n",
       "               74: 8,\n",
       "               108: 4,\n",
       "               127: 1,\n",
       "               152: 1,\n",
       "               149: 1,\n",
       "               68: 10,\n",
       "               103: 2,\n",
       "               85: 5,\n",
       "               10: 32,\n",
       "               138: 1,\n",
       "               144: 2,\n",
       "               82: 2,\n",
       "               70: 8,\n",
       "               3: 1,\n",
       "               125: 3,\n",
       "               105: 6,\n",
       "               84: 2,\n",
       "               99: 5,\n",
       "               98: 3,\n",
       "               102: 6,\n",
       "               153: 1,\n",
       "               122: 2,\n",
       "               190: 1,\n",
       "               136: 2,\n",
       "               92: 2,\n",
       "               114: 2,\n",
       "               175: 1,\n",
       "               143: 1,\n",
       "               96: 1,\n",
       "               91: 5,\n",
       "               90: 7,\n",
       "               101: 2,\n",
       "               100: 5,\n",
       "               111: 1,\n",
       "               197: 1,\n",
       "               129: 1,\n",
       "               110: 1,\n",
       "               145: 2,\n",
       "               156: 1,\n",
       "               135: 1,\n",
       "               162: 1,\n",
       "               83: 3,\n",
       "               113: 1,\n",
       "               168: 1,\n",
       "               126: 2,\n",
       "               94: 4,\n",
       "               104: 2,\n",
       "               120: 1,\n",
       "               128: 1,\n",
       "               133: 1,\n",
       "               115: 1,\n",
       "               124: 1,\n",
       "               97: 1,\n",
       "               163: 1,\n",
       "               232: 1,\n",
       "               106: 1,\n",
       "               130: 1,\n",
       "               117: 1}),\n",
       "  defaultdict(int,\n",
       "              {38: 34,\n",
       "               29: 35,\n",
       "               36: 23,\n",
       "               27: 55,\n",
       "               91: 2,\n",
       "               30: 31,\n",
       "               33: 31,\n",
       "               40: 19,\n",
       "               6: 3,\n",
       "               9: 14,\n",
       "               55: 12,\n",
       "               43: 19,\n",
       "               54: 8,\n",
       "               49: 14,\n",
       "               23: 30,\n",
       "               14: 21,\n",
       "               20: 25,\n",
       "               39: 27,\n",
       "               28: 41,\n",
       "               16: 35,\n",
       "               50: 6,\n",
       "               79: 3,\n",
       "               24: 32,\n",
       "               22: 37,\n",
       "               25: 35,\n",
       "               88: 3,\n",
       "               52: 14,\n",
       "               19: 31,\n",
       "               17: 28,\n",
       "               26: 37,\n",
       "               34: 25,\n",
       "               32: 33,\n",
       "               21: 27,\n",
       "               37: 23,\n",
       "               46: 15,\n",
       "               193: 1,\n",
       "               47: 11,\n",
       "               44: 13,\n",
       "               11: 24,\n",
       "               69: 6,\n",
       "               41: 29,\n",
       "               62: 5,\n",
       "               45: 17,\n",
       "               53: 14,\n",
       "               15: 29,\n",
       "               64: 10,\n",
       "               12: 19,\n",
       "               7: 5,\n",
       "               48: 12,\n",
       "               59: 5,\n",
       "               73: 5,\n",
       "               60: 8,\n",
       "               13: 26,\n",
       "               75: 3,\n",
       "               72: 8,\n",
       "               76: 4,\n",
       "               31: 31,\n",
       "               51: 9,\n",
       "               35: 25,\n",
       "               80: 5,\n",
       "               18: 24,\n",
       "               63: 6,\n",
       "               86: 2,\n",
       "               10: 14,\n",
       "               42: 27,\n",
       "               110: 3,\n",
       "               78: 1,\n",
       "               8: 5,\n",
       "               56: 11,\n",
       "               61: 9,\n",
       "               5: 1,\n",
       "               99: 2,\n",
       "               70: 11,\n",
       "               74: 6,\n",
       "               58: 9,\n",
       "               65: 8,\n",
       "               147: 2,\n",
       "               68: 6,\n",
       "               100: 3,\n",
       "               71: 7,\n",
       "               175: 1,\n",
       "               105: 2,\n",
       "               108: 2,\n",
       "               117: 2,\n",
       "               115: 1,\n",
       "               57: 8,\n",
       "               156: 1,\n",
       "               66: 3,\n",
       "               77: 3,\n",
       "               90: 3,\n",
       "               89: 2,\n",
       "               84: 3,\n",
       "               82: 4,\n",
       "               116: 1,\n",
       "               123: 1,\n",
       "               126: 1,\n",
       "               85: 2,\n",
       "               141: 1,\n",
       "               87: 1,\n",
       "               96: 2,\n",
       "               83: 2,\n",
       "               95: 1,\n",
       "               92: 1,\n",
       "               135: 1,\n",
       "               119: 1,\n",
       "               67: 2,\n",
       "               113: 1,\n",
       "               120: 1,\n",
       "               127: 2,\n",
       "               114: 1,\n",
       "               98: 1,\n",
       "               81: 1,\n",
       "               104: 1,\n",
       "               4: 1})]}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = {}\n",
    "for key in ['elmo_index', 'shell_span']:\n",
    "    count[key] = [collections.defaultdict(int), collections.defaultdict(int)]\n",
    "    \n",
    "for side in range(2):\n",
    "    for data in prepare_data[side]:\n",
    "        for key in ['elmo_index', 'shell_span']:\n",
    "            l = len(data[key])\n",
    "            count[key][side][l] +=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i believe everyone's opinion deserves equal respect, even if i personally find their opinion ridiculous.\n",
      "['Harlequnne', '[deleted]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'bio': ['B I I I I E O B I I I I I E O B I I I I I I I I I I I I I I I I E O B I I I I I E O O B I I I I I I I I I E O',\n",
       "   'B I I I I I I I I I I I I E O B I I I I I I I I I I I I I I I I I I E O O O O O O O O O O O O O O O O O O O O O O',\n",
       "   'B I I I I I I I E O O B I I I I I I I I I I I I I I I I E O O O O O O O O O O O O O O O O',\n",
       "   'O O O O O B I I I I I I I I I I I I I I I I I I E O O O O O'],\n",
       "  'context': [\"people hold lots of different views . it ' s part of being individuals . respect for those different views is , to me , one of the cornerstones of a civilized society . we are never all going to agree , but that does not mean we ca n ' t be respectful .\",\n",
       "   'i \\' ve noticed a trend that , to me , is very disturbing . many of my friends , including my fiance , find it completely acceptable to mock those with a differing viewpoint . \" how else will they learn ? \" they say , which sounds like a cop - out to me .',\n",
       "   'this is most specifically prevalent among christians and atheists , but it does not really matter to me who the dissenters are , only how they treat each other . i want to understand this attitude that seems to be more and more popular .',\n",
       "   'so , i think that both sides of an argument should be treated with equal respect , however ridiculous those sides may find one another . change my view ?'],\n",
       "  'author': 'Harlequnne',\n",
       "  'type': ['P', 'P', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']},\n",
       " {'bio': ['O O B I I I I E O O B I I I I I I I I I I I I I I I I E O',\n",
       "   'O O O B I I I I I I I I E O B I I I I E O O B I I I I I I I I I I I I I I I E O',\n",
       "   'O O O B I I I I I I I I I I I I I I I I I I I I E O',\n",
       "   'O O O O O B I I I E O O O O B I I I I I I I I I I I I I I I I I I I E O'],\n",
       "  'context': ['i think genuine opinions are worthy of respect ; but a lot of people hold patently false beliefs and try to defend them as if they were opinions .',\n",
       "   '\" i think crystal castles is a great band \" is an opinion . no one can argue with it , and it \\' s just as valid as \" i think crystal castles is a shitty band \" .',\n",
       "   '\" i think evolution is false \" is not an opinionit \\' s a false belief for which there is a mountain of evidence against .',\n",
       "   \"so while i agree that true opinions should be respected , i think that it ' s worth noting the difference between genuine opinions , and falsifiable beliefs being held as if they were opinions .\"],\n",
       "  'author': '[deleted]',\n",
       "  'type': ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']},\n",
       " {'bio': ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O'],\n",
       "  'context': [\"<cite> and i mention the same thing i did above . ' i think all gays should be killed ' is an opinion , not a false belief . yet i would never respect that opinion . do you think it should be respected ?\"],\n",
       "  'author': 'whiteraven4',\n",
       "  'type': []}]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "side, index = 1, 1233\n",
    "print(posts[side][index]['topic'])\n",
    "print(posts[side][index]['pos_author'])\n",
    "posts[side][index]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['content', 'topic', 'uid', 'pos_author'])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 48, 2),\n",
       " (1, 96, 2),\n",
       " (1, 126, 7),\n",
       " (1, 157, 2),\n",
       " (1, 355, 2),\n",
       " (1, 938, 2),\n",
       " (1, 959, 2),\n",
       " (1, 989, 8),\n",
       " (1, 1233, 3),\n",
       " (1, 1457, 5),\n",
       " (1, 1500, 4),\n",
       " (1, 1534, 2),\n",
       " (1, 1758, 5),\n",
       " (1, 1806, 2),\n",
       " (1, 1955, 2),\n",
       " (1, 1977, 2),\n",
       " (1, 2024, 2),\n",
       " (1, 2076, 6),\n",
       " (1, 2129, 2),\n",
       " (1, 2263, 4),\n",
       " (1, 2284, 8),\n",
       " (1, 2326, 2),\n",
       " (1, 2367, 2),\n",
       " (1, 2427, 5),\n",
       " (1, 2455, 2),\n",
       " (1, 2522, 2),\n",
       " (1, 2528, 2),\n",
       " (1, 2585, 6),\n",
       " (1, 2615, 8),\n",
       " (1, 2712, 8),\n",
       " (1, 2727, 5),\n",
       " (1, 2757, 2),\n",
       " (1, 2769, 5),\n",
       " (1, 2837, 2),\n",
       " (1, 2886, 3),\n",
       " (1, 3123, 8),\n",
       " (1, 3145, 8),\n",
       " (1, 3196, 2),\n",
       " (1, 3240, 2),\n",
       " (1, 3242, 2),\n",
       " (1, 3329, 4),\n",
       " (1, 3397, 4),\n",
       " (1, 3417, 2),\n",
       " (1, 3607, 2),\n",
       " (1, 3673, 3),\n",
       " (1, 3799, 2),\n",
       " (1, 4016, 4),\n",
       " (1, 4344, 2),\n",
       " (1, 4382, 8),\n",
       " (1, 4386, 2),\n",
       " (1, 4395, 2),\n",
       " (1, 4425, 4),\n",
       " (1, 4752, 6),\n",
       " (1, 4796, 8),\n",
       " (1, 4820, 2),\n",
       " (1, 4833, 6),\n",
       " (1, 4865, 4),\n",
       " (1, 4984, 9),\n",
       " (1, 5122, 9),\n",
       " (1, 5178, 2),\n",
       " (1, 5261, 2),\n",
       " (1, 5293, 2)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "for side in range(2):\n",
    "    for index, post in enumerate(posts[side]):\n",
    "        if(('[deleted]' in post['pos_author']) and post['content'][-1]['author'] == '[deleted]'):\n",
    "            c.append((side, index, len(post['content'])))\n",
    "        elif('[deleted]' in post['pos_author']):\n",
    "            c.append((side, index, len(post['content'])))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for side in range(2):\n",
    "    for index, data in enumerate(prepare_data[side]):\n",
    "        if((len(data['para_pos_author']) == sum(data['para_pos_author'])) or sum(data['para_pos_author'])==0):\n",
    "            print(len(posts[side][index]['content']))\n",
    "            print(side, data['uid'], index)\n",
    "            print(data['para_author'], len(data['para_author']))\n",
    "            print(data['elmo_index'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<para> as an aside , <ac> i would caution against evaluating concepts like purpose or meaning from a single person ' s conscious perspective </ac> . certainly <ac> we ' ve learned enough to discredit many philosophical ideas of the past , like randomly believing in an afterlife or blindly following churches and holy books </ac> . the undeniable fact remains that <ac> we do not understand everything </ac> . <ac> is it not possible that the experience we have as individuals is part of a greater collective that we don ' t have the capacity to understand </ac> ? </para>\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmv_text[0][5371]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bio': [None, None],\n",
       " 'context': [\"while i am not at all for the scare mongering and paranoia , i simply think that there is not enough evidence to show that gmos are completely safe to completely accept them with no questions asked . it seems very suspicious to me that large companies are lobbying so hard to avoid having to label gmos , as i do n't see why they would put so much time or money into fighting it unless there was something they were trying to hide . i have always tended to be wary of large corporations and their practices , but i feel like my current view is riddled with ignorance and misinformation . change my view ?\",\n",
       "  '<cite>'],\n",
       " 'author': 'MagicSpaceMan',\n",
       " 'type': [None, None],\n",
       " 'topic': 'gmos are a relatively new development, and should be labeled and treated with caution',\n",
       " 'pos_author': [[['MagicSpaceMan'], ['MagicSpaceMan']],\n",
       "  [['MagicSpaceMan', 'MagicSpaceMan'], ['MagicSpaceMan', 'MagicSpaceMan']]]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[236]['op_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bio': [None, None, None],\n",
       "  'context': [\"so what do you mean by safety ? to consume ? this is really easy to determine . the chemicals which constitute food plants are really well known and we can test a plant for anything we want . so if you want to know if a food is safe to eat , send it to some chemists , and they 'll tell you if it has any dangerous concentrations of chemicals .\",\n",
       "   \"you don't need to do a complex wait and see approach to it . you just use mass spectrometry. <link> there are other tests too , spectrometry is just the best at catching even traces .\",\n",
       "   'as far as labeling , they fight it because people would buy less if they saw the label .'],\n",
       "  'author': 'huadpe',\n",
       "  'type': [None, None, None]}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[236]['content'][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic_index': 5369,\n",
       " 'elmo_index': [5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377],\n",
       " 'shell_span': [[0, 0, 0],\n",
       "  [0, 15, 15],\n",
       "  [0, 30, 33],\n",
       "  [1, 0, 0],\n",
       "  [1, 25, 30],\n",
       "  [1, 50, 50],\n",
       "  [2, 0, 0],\n",
       "  [2, 34, 35],\n",
       "  [2, 66, 68],\n",
       "  [3, 0, 2],\n",
       "  [3, 17, 18],\n",
       "  [3, 36, 38],\n",
       "  [4, 0, 1],\n",
       "  [4, 16, 18],\n",
       "  [5, 0, 4],\n",
       "  [5, 25, 26],\n",
       "  [5, 38, 38],\n",
       "  [5, 56, 59],\n",
       "  [5, 119, 119],\n",
       "  [6, 0, 4],\n",
       "  [6, 23, 23],\n",
       "  [6, 74, 76],\n",
       "  [6, 99, 99],\n",
       "  [7, 0, 0],\n",
       "  [7, 10, 10]],\n",
       " 'span': [[0, 1, 14],\n",
       "  [0, 16, 30],\n",
       "  [0, 34, 42],\n",
       "  [1, 1, 25],\n",
       "  [1, 31, 49],\n",
       "  [1, 51, 84],\n",
       "  [2, 1, 33],\n",
       "  [2, 36, 65],\n",
       "  [2, 69, 77],\n",
       "  [3, 3, 16],\n",
       "  [3, 19, 36],\n",
       "  [3, 39, 46],\n",
       "  [4, 2, 16],\n",
       "  [4, 19, 32],\n",
       "  [5, 5, 25],\n",
       "  [5, 27, 37],\n",
       "  [5, 39, 55],\n",
       "  [5, 60, 118],\n",
       "  [5, 120, 136],\n",
       "  [6, 5, 22],\n",
       "  [6, 24, 74],\n",
       "  [6, 77, 98],\n",
       "  [6, 100, 125],\n",
       "  [7, 1, 9],\n",
       "  [7, 11, 19]],\n",
       " 'mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       " 'author': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'para_author': [1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'adu_label': [0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'ac_position_info': [[0, 0],\n",
       "  [1, 0],\n",
       "  [2, 0],\n",
       "  [3, 0],\n",
       "  [4, 0],\n",
       "  [5, 0],\n",
       "  [6, 0],\n",
       "  [7, 0],\n",
       "  [8, 0],\n",
       "  [9, 0],\n",
       "  [10, 0],\n",
       "  [11, 0],\n",
       "  [12, 0],\n",
       "  [13, 0],\n",
       "  [0, 1],\n",
       "  [1, 1],\n",
       "  [2, 1],\n",
       "  [3, 1],\n",
       "  [4, 1],\n",
       "  [5, 1],\n",
       "  [6, 1],\n",
       "  [7, 1],\n",
       "  [8, 1],\n",
       "  [9, 1],\n",
       "  [10, 1]],\n",
       " 'uid': '236_0'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(2>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14120, 27]\n",
      "[5629, 6]\n"
     ]
    }
   ],
   "source": [
    "for side in range(2):\n",
    "    c = [0, 0]\n",
    "    for key, val in count['elmo_index'][side].items():\n",
    "        c[int(key>81)] += val\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert tree version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load posts if needed\n",
    "posts = []\n",
    "for i, key in enumerate(['pos', 'neg']):\n",
    "    with open('./../preprocess/cmv_raw_origin_v2/heldout/tree/clean_post_{}.jsons'.format(key)) as f:\n",
    "        posts.append([json.loads(line) for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['content', 'topic', 'uid', 'pos_author'])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process link \n",
    "def find_shell_bert(index, last, context, adu_index):\n",
    "    if(index==0):\n",
    "        return (adu_index, 0, 0)\n",
    "    for i in range(index, last, -1):\n",
    "        if(context[i] in [\".\", \"!\", \"?\", \"</ac>\", \"<para>\"]):\n",
    "            if(i>=index):\n",
    "                return (adu_index, index, index)\n",
    "            else:\n",
    "                return (adu_index, i+1, index)\n",
    "    \n",
    "    return (adu_index, index, index)\n",
    "\n",
    "def prepare_bert(data):\n",
    "    mask = []\n",
    "    shell_span, span, text = [], [], []\n",
    "    adu_label = []\n",
    "    author = []\n",
    "    para_author = []\n",
    "    topic = data['topic']\n",
    "    \n",
    "    ac_position_info = []\n",
    "    adu_index = 0\n",
    "    for post_pos, post in enumerate(data['content']):\n",
    "        for bio, context in zip(post['bio'], post['context']):\n",
    "            text.append( context.split() )\n",
    "\n",
    "            bios, sent = bio.split(), context.split()\n",
    "\n",
    "            last = -1\n",
    "            for index, (bio, word) in enumerate(zip(bios, sent)):\n",
    "                if(bio == 'B'):\n",
    "                    span.append([adu_index, index, 0])\n",
    "                    shell_span.append(find_shell_bert(index-1, last, text[-1], adu_index))\n",
    "                \n",
    "                if(bio == 'E'):\n",
    "                    span[-1][-1] = index\n",
    "                    last = span[-1][-1]\n",
    "            \n",
    "            text[-1] = ' '.join(text[-1])\n",
    "            adu_index += 1\n",
    "        \n",
    "        # build mask\n",
    "        dtype = post['type']\n",
    "        prev_len, now_len = len(adu_label), len(dtype)\n",
    "        for adu_pos, _ in enumerate(dtype):\n",
    "            if(_ == 'P'):\n",
    "                # constrain search space to this reply\n",
    "                mask.append([0]*prev_len + [1]*now_len)\n",
    "\n",
    "            elif(_ == 'C'):\n",
    "                # constrain search space to all\n",
    "                mask.append([1]*(prev_len + now_len))\n",
    "\n",
    "            ac_position_info.append([adu_pos, post_pos])\n",
    "\n",
    "        adu_label.extend( dtype )\n",
    "\n",
    "    if(('[deleted]' in data['pos_author'][0]) and data['content'][-1]['author'] == '[deleted]'):\n",
    "        for index, post in enumerate(data['content']):\n",
    "            author.extend([1+index&1]*len(post['type']))\n",
    "            para_author.extend([1+index&1]*len(post['bio']))\n",
    "    else:\n",
    "        for post in data['content']:\n",
    "            if(post['author'] in data['pos_author'][0]):\n",
    "                author.extend([1]*len(post['type']))\n",
    "                para_author.extend([1]*len(post['bio']))\n",
    "            elif(post['author'] in data['pos_author'][1]):\n",
    "                author.extend([2]*len(post['type']))\n",
    "                para_author.extend([2]*len(post['bio']))\n",
    "            else:\n",
    "                author.extend([0]*len(post['type']))\n",
    "                para_author.extend([0]*len(post['bio']))\n",
    "    adu_map = {'P':0, 'C':1}\n",
    "    adu_label = [ adu_map[_] for _ in adu_label] \n",
    "        \n",
    "    return {\n",
    "        'topic':topic, \n",
    "        'text':text,\n",
    "        'shell_span':shell_span, \n",
    "        'span':span, \n",
    "        'mask':mask, \n",
    "        'author':author,\n",
    "        'para_author':para_author,\n",
    "        'adu_label':adu_label,\n",
    "        'ac_position_info':ac_position_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data = [[], []]\n",
    "\n",
    "for side in range(2):\n",
    "    for _ in posts[side]:\n",
    "        temp = prepare_bert(_)\n",
    "        temp['uid'] = _['uid']\n",
    "        \n",
    "        prepare_data[side].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(['pos', 'neg']):\n",
    "    with open('./../preprocess/cmv_raw_origin_v2/heldout/tree_bert/pre_{}.jsons'.format(key), 'w') as f:\n",
    "        for _ in prepare_data[i]:\n",
    "            f.write(json.dumps(_))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
